# 0c3a06aa State and Language Expressiveness in RNNs

## Content
State expressiveness in RNNs refers to the functions their hidden states can encode, while language expressiveness pertains to the languages they can recognize with a decoder. The study reveals hierarchies within RNN architectures: LSTMs can encode functions outside the realm of rational recurrences, while Elman RNNs and GRUs are within the rational recurrence domain but with constant space complexity. QRNNs possess unbounded space complexity with rational recurrences. Language recognition involves coupling an encoder with a decoder, showing how models like LSTMs can recognize sequences like a[n]b[n] more efficiently than QRNNs given their different expressiveness capacities.

## Metadata
- **Created**: 2024-10-27 19:22:12.394200
- **Tags**: #RNN, #State expressiveness, #Language recognition, #Hierarchies

## Links
<!-- Add links to related notes here -->

## References
<!-- Add references or sources here -->

