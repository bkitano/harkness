### Erratum Note

On October 6, 2023, Benjamin Spector and Emmanuel Chemla found a mistake in the main result
(Theorem 2) of this paper.[1] The technical problem
leads to an alternate interpretation of our results.
Our distributional “entailment test” does not actually detect entailment, but, rather, detects either
entailment or near contradiction, meaning two sentences are only consistent in a rare set of worlds,
without distinguishing which.


**Implications**

We believe near contradiction may be rare compared to entailment, so the test may still tend to
identify entailment correctly assuming realistic
data distributions. However, our paper’s main goal
was never to propose a practical NLI method but
rather to make the theoretical claim that mastering
the LM objective perfectly implies acquiring a full
model of entailment. The inability of our distributional entailment test to distinguish entailment
from near contradiction means the reconstruction
of semantics it would extract from an idealized,
perfect LM could still be fundamentally lossy. Future work should investigate whether distributional
semantics must fundamentally confuse entailment
and near contradiction or whether there is some
other way to distinguish them with form alone.


Figure 1: The entailment test score between x and y as
a function of the number of worlds where x is true but
_y is false. Entailment (left intercept with 0) and near_
contradiction (right intercept with 0) look the same!

**Detailed Problem and Revised Analysis**


**The Conceptual Problem**

The edge case that breaks Theorem 2 is simple to
describe conceptually. Our entailment test attempts
to use the co-occurrence probability p(xy) of two
sentences x, y to infer something about their semantic relationship. Under a Gricean speaker, the
probability of a redundant pair of utterances x, y
should be 0 and the probability of contradictory
_∼_
utterances x, y should be exactly 0. The issue is
when y nearly contradicts x, e.g.:


The technical problem in the original proof of Theorem 2 was that Lemma 1 was applied with unmet
preconditions. Specifically, it assumed that the
speaker utility Iℓ(z; w) is at least 0 for all utterances z and worlds w, but, in fact, this utility is
in worlds where z is false. Near contradictions
_−∞_
can then achieve an average exponentiated information content of 1 because exp( ) in many
_−∞_
worlds is balanced out by large positive information in a few worlds. Formally, let Y = _x_ _y_
� � _∩_ � �
be the worlds where x, y are both true. We show in
§H that the original entailment test is 0 when

_p(Y )I(Y ) = I,_ (1)


_x = I’m not in North America._


_y = I’m in a US state._

_y nearly contradicts x because they are both satisfi-_
able only in worlds where the speaker is in Hawaii,
which we assume p(w) makes unlikely. In such
instances of near contradiction, it is possible that
_p(xy) is slightly above 0 (like for entailment), and_
thus Theorem 2 detects entailment incorrectly.

1The authors thank Sophie Hao, Noah A. Smith, and
Zhaofeng Wu for feedback on this erratum.


where I(Y ) measures the information content of y
given x and I is a constant reflecting 0 information.
We can see that (1) has two distinct solutions:

**Entailment Solution.** As expected, (1) is satisfied when x entails y: p(Y ) = 1 and I(Y ) = I.

**Near-Contradiction Solution.** Assume for simplicity that Iℓ(y | x; w) = c for all w ∈ _Y . If y_
nearly contradicts x, p(Y ) is small because there
are very few contexts where x, y are both true. On
the other hand, I(Y ) is large because y is very informative when it is true. It is possible to calibrate
_Y such that these factors multiply to I._
Figure 1 illustrates these two solutions using the
Gricean speakers from §6. For two utterances x, y,
we vary the number of worlds where x is true but
_y is false, ranging from entailment to contradiction._
The test score crosses 0 twice: for entailment on
the left and near contradiction on the right.


-----

## Entailment Semantics Can Be Extracted from an Ideal Language Model

### William Merrill Alex Warstadt Tal Linzen New York University ETH Zürich New York University {willm,linzen}@nyu.edu alexanderscott.warstadt@inf.ethz.ch


### Abstract

Language models are often trained on text
alone, without additional grounding. There is
debate as to how much of natural language semantics can be inferred from such a procedure.
We prove that entailment judgments between
sentences can be extracted from an ideal language model that has perfectly learned its target
distribution, assuming the training sentences
are generated by Gricean agents, i.e., agents
who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can
be decoded from the predictions of a language
model trained on such Gricean data. Our results
reveal a pathway for understanding the semantic information encoded in unlabeled linguistic
data and a potential framework for extracting
semantics from language models.

### 1 Introduction

Recent advances in building computational models
of language have been powered by distributional
_semantics: the idea that the possible surrounding_
contexts for a text span encode its meaning (Firth,
1957). In particular, large pretrained language models (LMs; Peters et al., 2018; Devlin et al., 2019;
Brown et al., 2020) have become an integral part of
NLP systems: the representations that emerge from
training to predict missing words in a text are empirically useful for natural language understanding
tasks.
Despite this empirical progress, Bender and
Koller (2020) argue LMs cannot learn to understand the semantics of sentences. This is because
of a mismatch between the LM training objective—
predicting missing words in text (“form”)—and
Bender and Koller’s conception of meaning as the
relation of a sentence to the external world. Thus
Bender and Koller claim “that the language modeling task, because it only uses form as training data,
cannot in principle lead to learning of meaning.”


In this paper, we argue meaning can be learned
from form because the communicative goals of human authors encode semantic information in unlabeled text. We show how this semantic information
can be extracted to resolve semantic relations between sentences (e.g., whether one sentence entails
another): in this inferentialist sense, ideal LMs encode the meaning of sentences. This argument has
been raised speculatively by others (Michael, 2020;
Potts, 2020; Bommasani et al., 2021), but we will
rigorously justify it here with formal results.
To give the simplest (and least general) illustration of our argument, we first assume training data
is generated by overly idealized uniformly truthful
speakers: agents who decide what to say by picking
sentences they consider true uniformly at random.[1]

This very coarsely captures human authors’ goal of
being informative (rather than misleading) to their
listeners (Grice, 1975). In Theorem 1, we prove a
sentence x entails sentence y if and only if, after
uttering x, a uniformly truthful speaker is just as
likely to say y as to repeat x. Thus, entailment
semantics can be extracted from probabilistic languages generated by uniformly truthful speakers.
Uniformly truthful speakers are not a realistic
model of humans: while humans favor true sentences to false ones (Grice, 1975), not all true sentences are equally likely to be produced. It is a
common principle in linguistic theories of pragmatics that human speakers choose their utterances in
order to balance two competing objectives: (a) conveying information to their listener and (b) brevity
(Levinson et al., 1983; Grice, 1975). We define a
class of Gricean speakers who optimize for these
objectives, and prove in Theorem 2 that x entails y
if and only if a simple equation holds in terms of
text probabilities produced by such speakers. Thus,

1Studying the ability of LMs to understand programming
language semantics, Merrill et al. (2021) make a similar assumption that programmers are more likely to write true assertion statements than false ones.


-----

entailment semantics can be decoded from probabilistic languages generated by Gricean speakers.

The previous results assume access to a language’s ideal likelihood function, but, in practice,
one only ever receives a corpus sampled from the
language. Moving to the corpus setting, we analyze
how much data allows approximately computing
our derived entailment test using probabilities estimated from sentence frequencies in a corpus. We
find that the corpus size needed to guarantee the
entailment test holds approximately is inversely
related to the likelihood of the sentences. We estimate that approximating the entailment test between 4-word sentences using corpus frequencies
is possible with 10[10] sentences, about the size
_∼_
of the GPT-3 training data (Brown et al., 2020).
On the other hand, approximating the entailment
test for 10-word sentences should be possible with
10[17] sentences, or 10[7] GPT-3 corpora. Thus,
_∼_ _∼_
extracting entailment judgments using corpus frequencies requires an infeasible amount of data—
even by modern NLP standards.

To overcome this limitation, one might hope to
use probabilities estimated by LMs to extract entailment judgments between longer sentences that
are rare even in a large corpus. With synthetic data
generated by Gricean speakers, we find that entailment can be decoded from n-gram LM predictions
to some extent. However, we speculate that current
neural LMs may not score the probability of rare
text well enough to enable decoding entailment
judgments between natural language sentences.

In summary, our main contribution is to show a
correspondence between the semantics of text and
its likelihood, assuming the likelihood function
matches models of human text production from linguistic theory. Determining whether a sentence in a
probabilistic language entails another sentence can
be reduced to modeling the probabilities of strings
in the language. In practice, entailment judgments
between very short sentences can be extracted from
corpus frequencies, but this becomes infeasible for
slightly longer sentences. LMs can in principle be
used to extrapolate the likelihood of longer strings,
but we hypothesize current LMs are not well-suited
for doing so well enough to enable extracting entailment from natural language. Our theory demonstrates a formal sense in which unlabeled text data
encodes linguistic meaning and makes quantitative
predictions for (a) how to extract semantics from
text corpora and (b) how much data this requires.


### 2 Definitions

**2.1** **Sentences and Worlds**

Let be a finite set of sentences, and a count_X_ _W_
able[2] set of possible world states. A sentence x is
a string whose denotation _x_ is a proposition, i.e.,
� �
a set of world states ( ) where x is true. Fol_⊆W_
lowing standard conventions in formal semantics
(cf. Heim and Kratzer, 1998), the set _x_ can be
� �
equivalently viewed as a function mapping a world
state w to 0, 1 that indicates whether x is true in
_{_ _}_
_w, which we will write as_ _x_ (w). We imagine w
� �
to encode a partial description of the world, much
like the concept of a situation in formal semantics
(Kratzer, 2021). For simplicity, we assume an individual’s subjective belief state can be modeled
as the unique, maximal w that fully describes the
facts which they believe to be true.

**Example** _x = John has at least two cats._
Let W = {w0, w1, w2, w3} be the set of possible
worlds, where wn denotes the state in which John
has n cats. Then �x� = {w2, w3}, because John
has at least two cats in these worlds. Furthermore,
it holds that �x�(w2) = 1, but �x�(w1) = 0.

**2.2** **Speakers and Texts**

We refer to a sequence of sentences z ∈X _[∗]_ as a
_text.[3]_ The meaning of a text is the set of worlds
consistent with all its sentences, i.e.,


The conditional probability of an incomplete text
represents the probability of observing z as the

2Our results extend to uncountable sets of world states if
entailment is relaxed to hold almost surely (cf. §B). Alternatively, our results apply as-is if we assume a countable set of
equivalence classes over uncountably many worlds.
3Where X ∗ denotes the Kleene star closure of X .


_z_ =
� �


_|z|_
�

�zt�.
_t=1_


We will imagine that a text z ∈X _[∗]_ is produced by
iteratively sampling zt ∈X ∪{$} from a speaker
_model p(zt | z<t, w). p(zt | z<t, w) represents the_
probability of saying sentence zt with belief state
_w after having said z1 · · · zt−1. Let $ ̸∈X be a_
special end of sequence token satisfying $ = .
� � _W_
We refer to any text ending with $ as complete.
Given a world w, an incomplete text z ∈X _[∗]_ or
complete text z ∈X _[∗]$ has conditional probability_


_p(z_ _w) =_
_|_


_|z|_
�

_p(zt | z<t, w)._
_t=1_


-----

prefix of a text written by a human with beliefs
_w. In contrast, the probability of a complete text_
represents the probability that a speaker produces
_z and no further text. The conditional distribution_
_p(z_ _w) cannot be observed directly by a LM,_
_|_
since w is a latent variable missing from the training data. Rather, a LM has access to texts that have
been generated by speakers across many possible
belief states. Mathematically, this can be expressed
by saying a LM’s target distribution is a marginal
distribution over z ∈X _[∗]_ _∪X_ _[∗]$ according to some_
_prior distribution over worlds p(w):_

_p(z) =_ E
_w∼p(w)_ [[][p][(][z][ |][ w][)]]

� _∞_ �
�

= _w∼Ep(w)_ _t=1_ _p(zt | z<t, w)_ _._

The prior p(w) represents the probability that a
speaker contributing to the corpus will have belief
state w—we make no assumptions about its form
besides that p(w) > 0 for all w, and, for ev_∈W_
ery sentence, there is some world state that makes
that sentence true. In contrast to p(z), which corresponds to the expected corpus frequency of z, we
denote by p( _z_ ) the probability that z is true.[4]
� �

**Example** Let z be the 2-sentence text:[5]

_z1 = We swung our swords._

_z2 = That was ever so long ago._

Let p be the distribution of all possible English
web texts. The marginal probability p(z) can be decomposed across many possible worlds. One such
world w1 might be the world where the speaker is
the semi-legendary Viking hero Ragnar Loðbrók
(in modern English translation); another world w2
might be the perspective of a Reddit user reviewing
a coffee maker. Each of these worlds corresponds
to one term in a sum over all worlds. We expect
_p(z | w1) to be higher than p(z | w2) since it_
is more likely for a medieval literary character to
utter z than a modern product reviewer. Finally,
_p(z | w1) can be factored as_

_p(z1 | w1)p(z2 | z1, w1)._

In contrast to p(z), which counts all contexts where
_z is the beginning of a longer text, p(z$) measures_
the frequency of z1z2 followed by nothing else.

4The notation explicitly represents the probability mass
assigned to the set of worlds where z is true.
[5Text taken from the Wikipedia page for the skaldic poem](https://en.wikipedia.org/wiki/Kr%C3%A1kum%C3%A1l)
_Krákumál, written in Ragnar’s voice._


**2.3** **Distributional and Semantic Relations**

**Distributional Relations** A distributional rela_tion d is a relation over sentences x and y defined_
in terms of likelihood of different texts under some
distribution p. Let dp(x, y) be the value of the
distributional relation d between sentences x, y according to distribution p. If we train an LM on
texts sampled from a target distribution p, the LM
estimates a predictive distribution ˆp. Thus, any LM
parameterizes dpˆ: an instantiation of the distributional relation d with respect to the probabilities
learned by the LM. If the LM perfectly approximates p(x) for all x, then dpˆ = dp by construction.

**Example** Define the distributional relation d
(with respect to some distribution p) such that
_d[>]p_ [(][x, y][)] _⇐⇒_ _p(x) > p(y). d[>]p_ [(][x, y][)][ says][ x]
is more likely than y according to p. If ˆp represents
LM predictions trained on the target distribution
_p, than d[>]pˆ_ [(][x, y][)][ says whether the LM predicts a]
sentence x is more likely than another sentence y.

**Semantic Relations** In contrast, a semantic relation between x and y is a relation defined in terms
of their denotations _x_ and _y_ . We will focus on
� � � �
the key semantic relation of entailment:

**Definition 1 For two sentences x, y**, x entails
_∈X_
_y if and only if_ _x_ _y_ .
� � _⊆_ � �

It is not clear prima facie if LMs can represent
entailment relations. However, it could be that a
semantic relation s can somehow equivalently be
written as a distributional relation dp. If so, a LM
that perfectly approximates p could be understood
to encode s, since s can be extracted from ˆp via dpˆ.
Formally, we can ask if a semantic relation can
be alternatively expressed as a distributional relation by analyzing if there exists an isomorphism
between a semantic relation s( _x_ _,_ _y_ ) and some
� � � �
distributional relation dp(x, y):

**Definition 2 (Isomorphism) A semantic relation**
_s is isomorphic to a distributional relation d under_
speaker p if and only if, for all x, y,
_∈X_

_s(�x�, �y�) ⇐⇒_ _dp(x, y)._

If Definition 2 holds under a speaker model p,
then predicting whether s holds between two sentences is reducible to perfectly modeling the probabilities of texts generated by p. Our goal going
forward will be to derive distributional relations
isomorphic to entailment assuming p models the
goals of humans when they produce text.


-----

### 3 Uniformly Truthful Speakers

We start by illustrating our research question and
technical approach assuming an overly simple
model of humans as uniformly truthful speakers.
A uniformly truthful speaker chooses a sentence
to produce by selecting one of the true sentences
that holds in their belief state uniformly at random. This very coarsely captures the property of
natural language pragmatics that subjectively true
sentences tend to be more likely than false ones,
although it does not account for many other factors
that influence human speech patterns in complex
ways (Grice, 1975).[6] Let n(w) be the number of
_sentences true in world w. We can formally define_
a uniformly truthful speaker as follows:

**Definition 3 A speaker p is uniformly truthful if,**
for all sentences x $,
_∈X ∪{_ _}_

_x_ (w)
_p(x_ _w) =_ � �
_|_ �x[′][�][x][′][�][(][w][) =][ �]n[x][�]([(]w[w])[)] _[.]_

In other words, p uniformly spreads probability
mass across all sentences that are true in world w.
We will show that, if the corpus consists of text
written by uniformly truthful speakers, entailment
can be decided by a distributional relation. The
following lemma will be a core technical tool in
our analysis. Informally, it is useful because it establishes a correspondence between relations over
sets of worlds and probabilities.

**Lemma 1 Let 1S be the indicator function for set**
_. For sets_ _,_ _such that_ _, and_
_S_ _A_ _B_ _A ⊆B ⊆W_
_c : W →_ R+, A = B if and only if
� 1 (w)c(w) = � 1 (w)c(w).

_A_ _B_
_w∈W_ _w∈W_

_Proof. We will prove that_ by contradiction.
_B ⊆A_
Assume there exists w such that w . Then
_∈B_ _̸∈A_
the right sum contains the positive term c(w), while
the left sum does not. Because all terms in the right
sum are positive, the left sum must contain at least
one term c(w[′]) that the right sum does not. Thus,
_w[′]_ _∈A but w[′]_ _̸∈B. But this has violated our_
assumption that .
_A ⊆B_

We now use Lemma 1 to derive a simple distributional relation that is isomorphic to entailment.

6LMs sometimes generate objectively false statements (Lin
et al., 2022), presumably due to the occurrence of such facts in
their training data. This is actually consistent with a uniform
truthfulness assumption, which only requires that speakers
only produce sentences they believe are true, not sentences
that are actually true in some objective sense.


An expectation in a countable space is a sum
weighted by probability masses. So, by Lemma 1,
this holds iff _x_ = _xy_ = _x_ _y_ . We conclude
� � � � � �∩� �
_p(xy) = p(xx) if and only if_ _x_ _y_ .
� � _⊆_ � �

A similar proof suffices to show that the following isomorphism also holds:

**Corollary 1.1 If p is a uniformly truthful speaker,**
_the following isomorphism holds for all x, y_ _:_
_∈X_

_x_ _y_ _p(xy) = p(x$)._
� � _⊆_ � � _⇐⇒_

**3.1** **Discussion**

Uniformly truthful speakers resemble humans in
that they mimic the tendency of humans to tell the
truth about what they believe. However, they are
clearly too simple to account for human speech
patterns. Most crucially, humans generally aim to
produce informative speech, rather than sampling
true sentences at random. More fundamentally,
natural language has a countably infinite number
of possible sentences, so a uniform distribution
over all true sentences is not even mathematically
well-defined. These limitations motivate our more
involved analysis of Gricean speakers, which will
adapt the technical tools used in this section.

### 4 Gricean Speakers

In this section, we will define a new class of speakers who pick sentences in order to be informative
to their listener, while also trying to be concise.
To do this, we will draw on information theory to
formalize what it means for a speaker to be informative. We will then derive a distributional relation
that is isomorphic to entailment for Gricean speakers, which is a generalization of the relation for
uniformly truthful speakers from §3.


**Theorem 1 If p is a uniformly truthful speaker,**
_then entailment is isomorphic to a distributional_
_relation. Specifically, for all sentences x, y_ _,_
_∈X_

_x_ _y_ _p(xy) = p(xx)._
� � _⊆_ � � _⇐⇒_

_Proof. dp(x, y) holds if and only if_

_p(xy) = p(xx)_


� _x_ (w) _x_ (w)
� � � �

_n(w)[2]_


� _x_ (w)
� �

_n(w)[2]_


�


E
_w_

E
_w_


� _x_ (w) _y_ (w)
� � � �

_n(w)[2]_

� _x_ (w) _y_ (w)
� � � �

_n(w)[2]_


�
= E
_w_

�
= E
_w_


�
_._


-----

**4.1** **Definition**

**Information** The first step towards formalizing
Gricean speakers is to define a notion of the semantic information contained in a sentence. We
formalize a listener ℓ(w _z) as the inverse of a_
_|_
speaker: Given a text z ∈X _[∗], a listener produces_
a distribution over possible world states. Then, in a
given world w we can define the information that a
text conveys to the listener as the reduction in the
number of bits needed to transmit w to ℓ after they
have read z compared to before they have read z.
**Definition 4 The information content of a text z**
_∈_
_X_ _[∗]_ _∪X_ _[∗]$ to a listener ℓ(w | z) is[7]_

_Iℓ(z; w) = log ℓ(w | z) −_ log ℓ(w).

In other words, the information content of a text
is the reduction in ℓ’s code length for the world
after having read the text compared to beforehand.
We can naturally extend Definition 4 to measure
the conditional information conveyed by sentence
_y given that x has already been produced:_
**Definition 5 The information content of y ∈X** _[∗]_ _∪_
_X_ _[∗]$ given x ∈X_ _[∗]_ to a listener ℓ(w | z) is

_Iℓ(y | x; w) = Iℓ(xy; w) −_ _Iℓ(x; w)_

= log ℓ(w _xy)_ log ℓ(w _x)._
_|_ _−_ _|_

**Informative Speaker** We now define a Gricean
speaker in terms of Iℓ. Our definition generalizes the rational speech acts model (Goodman
and Frank, 2016), but makes weaker assumptions
about the listener and allows a dynamic semantics
where later sentences can condition on previous
ones (Lewis, 1979; Kamp, 1981; Heim, 1982). We
define an utterance’s utility as a convex combination of its information content and its cost to
produce, operationalizing the Gricean idea that
speakers pick utterances by weighing their informativeness against their cost. The cost function c : X _[∗]_ _∪X_ _[∗]$ →_ R can be any measure
of sentence complexity (e.g., length) satisfying
_c(xy) = c(x) + c(y) for x, y ∈X_ _[∗]_ _∪X_ _[∗]$.[8]_

**Definition 6 A speaker p is Gricean if there exists**
a listener ℓ(w _z), some α > 0, and a cost function_
_|_
_c such that, for all z ∈X_ _[∗]_ _∪X_ _[∗]$:[9]_

_p(z | w) ∝_ exp (αIℓ(z; w) − _c(z)) ._

7For convenience, we let log 0 = −∞ and ∞−∞ = 0.
8This is satisfied when c(x) is the length of x, but also for
other options like the corpus frequency of x (Goodman and
Frank, 2016) or the depth of the syntactic tree of x.
9To clarify, we assume p(z | w) is locally normalized at
each sentence rather than globally. However, our results also
go through with global normalization as well.


Further, ℓ must satisfy the following for all x ∈X _[∗],_
_y_ $, and w,
_∈X ∪{_ _}_ _∈W_

_Iℓ(y | x; w) = 0 ⇐⇒_ �x�(w) → �y�(w).

In other words, the speaker must be trying to
convey information about the state of the world to
some listener who fully absorbs the semantic information in all sentences they have already heard:
clarifying already established information will not
benefit the listener. We can formalize this by deriving p(y | x, w) for x ∈X _[∗]_ and y ∈X ∪{$}:

_p(y_ _x, w) =_ _[p][(][xy][ |][ w][)]_
_|_

_p(x_ _w)_
_|_
_∝_ exp (αIℓ(y | x; w) − _c(y)) ._

Notably, the probability of y given x depends on the
_conditional information of y given x, which means_
only information conveyed by y that is nonredundant with x will make y more likely.[10]

**4.2** **Results**

Proofs are in §C. Under a Gricean speaker, the cost
of an utterance can be expressed:

**Lemma 2 For any Gricean speaker p and x** _,_
_∈X_

_p(x$)_
_p(xx) [= exp(]exp([c]c[(]($))[x][))]_ _[.]_

**Corollary 2.1 Under a Gricean speaker, for all**
_x_ _, c(x) = log p(x$)_ log p(xx) + c($).
_∈X_ _−_

Corollary 2.1 says that a sentence is costly to
the extent that it is unlikely to be repeated twice,
giving an intuitive characterization of this quantity
in terms of text probabilities. Now, we will use this
characterization of cost to derive a distributional
relation that is isomorphic to entailment.

**Theorem 2 Under any Gricean speaker p, en-**
_tailment is isomorphic to a distributional relation._
_Specifically, for all sentences x, y_ _,_
_∈X_

_x_ _y_
� � _⊆_ � � _⇐⇒_ _[p]p[(]([xy]x$) [)]_ [=][ p]p[(]([yy]y$)[)] _[.]_


If we allow our decision rule to depend on the
cost function c in addition to probabilities, we can
simplify Theorem 2 as follows:

10From a technical perspective, the exp in Definition 6
is justified by the fact that probabilities decompose multiplicatively, i.e., p(xy | w) = p(x | w)p(y | x, w), but the
information content and cost of text should decompose additively across different sentences. Applying basic exponent
rules shows that Definition 6 satisfies this desideratum.


-----

**Corollary 2.1 Under any Gricean speaker p, for**
_all sentences x, y_ _,_ _x_ _y_ _if and only if_
_∈X_ � � _⊆_ � �

log p(x$) log p(xy) = c(y) _c($)._
_−_ _−_

If we imagine c(y) _c($) = 0 for a uniformly_
_−_
truthful speaker, we see the equation in Theorem 2
is a generalization of the equation in Theorem 1.

**4.3** **Discussion**

Gricean speakers are a general enough model of
humans speakers to capture the basic pragmatic
principles influencing speech production. Thus, it
is notable that Theorem 2 establishes a closed-form
distributional relation isomorphic to entailment.
One conceptual limitation of Gricean speakers
is that their simulated listener must fully consume
information, such that redundantly conveying the
same information twice will not lead to any information gain the second time. This contrasts with
real speech, where potential interpretation errors by
the listener incentivize the speaker to be somewhat
redundant (Degen et al., 2019). Mathematically,
this would violate the axiom of Definition 6 that

_Iℓ(y | x; w) = 0 ⇐⇒_ �x�(w) → �y�(w).

Extending Theorem 2 to speakers who use redundancy to account for noise and interpretation errors
is an interesting direction for future work.
Another interesting extension would be formalizing speakers who aim to be informative regarding
some question under discussion, rather than being generally informative about w (cf. Goodman
and Lassiter, 2015). This could encompass both
“what” questions that aim to clarify some aspect of
the world, and “why” questions that aim to convey
explanations for established facts.

### 5 Decoding Entailment from Empirical Text Frequencies

We have so far shown that entailment judgments
can be extracted from the sentence probabilities
in the ideal distribution p(z). What happens if,
more practically, we estimate the probability of a
sentence by its frequency in a large corpus sampled
from p(z)? We prove this method enables feasible
extraction of entailment judgments between very
short sentences, but the corpus size may become
intractably large for longer sentences.
Imagine we have a finite corpus of iid sentences
_{Zi}i[n]=1[, each sampled from][ p][(][z][)][. Let][ ˆ][p][(][z][)][ be the]_


empirical frequency of a text z in the corpus, i.e., if
_π(z, z[′]) returns whether text z is a prefix of text z[′],_


_pˆ(z) = [1]_

_n_


_n_
�

_π(z, Zi)._
_i=1_


Since p(z) encodes entailment via our extraction rules, ˆp(z) will encode entailment between
sentences if ˆp(z) is close to p(z). A naive notion
of closeness is to guarantee, for all ϵ, there exists
some number of texts n such that, with high probability, _p(z)_ _pˆ(z)_ _< ϵ. But this notion is not_
_|_ _−_ _|_
strict enough: if p(z) is small, this difference will
also be small, even if ˆp(z) is not a good approximation of p(z) on a relative scale. Instead, we want to
guarantee that ˆp(z)/p(z) converges to 1, or, equivalently, that their difference as log probabilities
converges to 0. This ensures that convergence will
still be meaningful for low-probability sentences,
which most sentences are in natural language.
Under this standard, rarer sentences take more
samples to approximate. Define the sentence complexity Kp(z) = _p(1z)_ [. We bound the approximation]

error in terms of Kp(z).[11]

**Lemma 3 For z ∈X** _[∗]_ _∪X_ _[∗]$ and δ > 0, it holds_
_with probability at least 1_ _δ_ (1 _p(z))[n]_ _that_
_−_ _−_ _−_


�


Kp(z)

_δn [.]_


log p(z) log ˆp(z)
_|_ _−_ _| ≤_


To make this bound non-vacuous, n must be
large enough to counteract Kp(z) and bring (1 −
_p(z))[n]_ close to 0. Thus, good approximation requires fewer samples for more common sentences.
To get a more concrete view of the number of samples required to extract entailment judgments from
an LM, we analyze Kp(z) for Gricean speakers.[12]

Recall that we write c(z) for the cost that a
Gricean speaker assigns to producing a sentence z.
For Gricean speakers, Kp(z) is related to c(z) as
well as the probability z is true.
**Theorem 3 Assume that p(z** _w) is a Gricean_
_|_
_speaker with respect to listener ℓ_ _and_ _z_ (w) =
� �
1 ⇐⇒ _Iℓ(z; w) ≥_ 0. Let gp(x, y) = log _[p]p[(]([xy]x$)[)]_ _[−]_

log _[p][(][yy][)]_

_p(y$)_ _[. Let][ q][ = 1][ −]_ [min][{][p][(][xy][)][, p][(][yy][)][}][. Then,]
_for all x, y_ _such that_ _xy_ (p) > 0, for all
_∈X_ � �
_δ > 0, it holds with probability at least 1_ _δ_ 4q[n]
_−_ _−_

_that |gp(x, y) −_ _gpˆ(x, y)| is at most_


�

exp(max _c(xy), c(yy)_ )
_{_ _}_

8 [1]

_·_
_p(_ _xy_ ) _δn_ _[.]_
� �

11Omitted proofs from §5 are in §D.
12§D also analyzes uniformly truthful speakers.


-----

Figure 2: Estimated number of training sentences for
guaranteeing gpˆ closely approximates gp, where ˆp is
estimated using empirical text frequencies.

Theorem 3 says we can use text frequencies to
decode entailment between sentences x, y from a
Gricean corpus, but the number of training sentences to guarantee this grows exponentially with
the cost of x and y. Thus, we probably cannot
expect to extract entailment judgments from text
frequencies except between very short sentences.
We make this more quantitative in Figure 2,
where we estimate the number of training sentences
needed to ensure gp and gpˆ are close on sentences of
length _k as a function of k. The main assumption_
_≤_
behind this calculation is that a sentence’s probability vanishes exponentially in its length, where
the exponential base is the perplexity of the language. §E documents the underlying assumptions
in more detail. Figure 2 predicts gp and gpˆ can be
made close for length-4 sentences using 10[10]
_∼_

training sentences: about as much data as GPT-3
was trained on. In contrast, handling (still short)
sentences of length 10 can be done with 10[17]
_∼_

training sentences, or 10[7] GPT-3 corpora. Thus,
_∼_
relying solely on corpus frequencies is likely not
a feasible way to extract entailment relations from
text generated by Gricean speakers.


### 6 Decoding Entailment from LMs

We have just analyzed how many samples are necessary to decode entailment relations from the text
frequencies in a finite corpus. As shown by Theorem 3, this approach will require intractably many
samples for sentences of nontrivial length because
longer strings will appear infrequently (if at all)
in the corpus. In order to estimate the probability
of rare, longer, strings what if we use an LM to
estimate ˆp(z) instead of text frequencies? Perhaps
a smoothed LM should allow us to extrapolate ˆp(z)
well enough for long sentences to extract entail

ment judgments between them. In this section, we
briefly discuss some limitations of this approach.
It is tempting to take low LM perplexity as evidence that an LM estimates sentence probabilities
well enough to approximately satisfy the isomorphism in Theorem 2. After all, low test perplexity
implies that ˆp(z) is, on average, a good approximation of p(z): if the perplexity is bounded below ϵ,
then the KL divergence KL(p, ˆp) is bounded below
log ϵ. ϵ decreases with the amount of training data
_n at a rate between Ω(1/[√]n) and Ω(1/n) (Wang_
et al., 2013; Li and Liu, 2021). Thus, with enough
data, ˆp(z) will closely approximate p(z) for an
average sentence z in the training distribution.
But low error on an average z does not establish
entailment can be decoded from ˆp because dpˆ, as
derived in Theorem 2, depends on the text z = yy,
which is very unlikely in natural language.[13] Poorly
estimating p(yy) has little impact on KL(p, ˆp), so
LMs trained to minimize KL(p, ˆp) have no reason
to estimate p(yy) well unless they are imbued with
strong inductive biases. Thus, we expect that LMs
trained with a standard cross-entropy loss may not
produce reliable entailment judgments because they
poorly estimate the probability of key valid (but unlikely) texts.[14] However, we find in the next section
that they do succeed in the easier setting of small
artificial languages and fully Gricean speakers.

### 7 Experiments: Extracting Semantics from Simulated Gricean Corpora

We test empirically whether we can extract entailment judgments from LMs trained on unlabelled
text.[15] Natural language corpora are unlikely to adhere exactly to our idealized assumptions about the
speakers generating texts, so we generate the training corpora from a simulated Gricean speaker (see
§4). To make learning semantics more tractable
with limited computation, we set = 3 and
_|W|_
restrict the vocabulary to 7 utterances, each de_X_
noting one of the 7 non-empty subsets of . Each
_W_
sentence in the training corpus is generated by sampling utterances from a Gricean speaker, conditioned on a uniformly sampled world state and the

13yy is unlikely to be produced by a Gricean speaker because the second y conveys no information.
14Future work should more carefully analyze how much
data is required to extract complex entailment relations from
LM predictions (rather than corpus frequencies). This is beyond the scope of the current project.
[15https://github.com/viking-sudo-rm/](https://github.com/viking-sudo-rm/formal-language-understanding)
[formal-language-understanding](https://github.com/viking-sudo-rm/formal-language-understanding)


-----

previously generated utterance, until the tautological utterance is generated. The semantic value of a
sentence is taken to be the conjunction over all of
its utterances. We set the rationality parameter α
and the cost function heuristically (details in §G).
We generate training sets varying in size from
2 texts to 10M texts, and train two types of models on each: a simple empirical text frequency as
described in Section 5, and a trigram model implemented using NLTK (Bird, 2006). Then for
all sentence pairs (x, y), where x and y have 6 utterances or fewer and each denotes a non-empty
proposition, we compute gpˆ(x, y) from §5. Theorem 2 shows that, under the true distribution p,
_gpˆ(x, y) = 0 if and only if x entails y._
The results are plotted in Figure 3. We arrive at
the following conclusions:

**Entailment relations can be extracted with**
**greater-than-chance performance from LM pre-**
**dictions.** The value of gpˆ(x, y) is much closer to
0 on average for entailed pairs than for non-entailed
pairs. This is predicted by Theorem 2.

**The size of the corpus needed to extract entail-**
**ment grows predictably with sentence length.**
For entailed pairs, the average value of gpˆ(x, y) for
shorter sentences approaches 0 more quickly with
a large training corpus. This is in line with the
predictions of Theorem 4.

**Model inductive bias impacts the ease of extract-**
**ing entailment.** Entailed and non-entailed pairs
are better distinguished by the trigram model than
the text frequency model. Specifically, gpˆ(x, y) is
closer to 0 for the trigram model for a given amount
of data, and the trigram model’s predictions are less
sensitive to sentence length.

### 8 Generality of Extracting Semantics

Our main result that entailment judgments can be
extracted from an ideal LM assumes the corpus
was produced by Gricean speakers. While pragmatic theory supports this assumption, real human
speakers are undoubtedly more complex. What if
we relax the assumption that speakers are Gricean?
In Theorem 6 in §F, we show that any semantic
relation is isomorphic to some distributional relation as long as, for any pair of possible semantics,
there is some text whose probability distinguishes
between the two candidate semantics.
We take it to be uncontroversial that semantics
influences speech production, so we interpret Theo

rem 6 to say all semantic relations are fully encoded
in ideal LMs. In contrast to Theorem 2, however,
this result is nonconstructive, so we do not know
which algorithm to use to decide entailment between two sentences, even though one exists. Further, without further assumptions about the speaker,
we cannot guarantee the extraction relation is efficiently computable or even computable at all.

### 9 Conclusion

Given a general, linguistically motivated model of
human text production, we proved that entailment
judgments can be decoded from the likelihood function for texts because of semantic artifacts created
by human authors. We also showed empirically that
entailment could be extracted n-gram LMs trained
on simple formal languages. Thus, we have given
one explanation for why distributional information
encodes semantic information (Firth, 1957) and
how semantic relations are, in principle, extractable
from LMs. It is an open question whether entailment judgments might be extractable from current
large LMs, but we hypothesize that the complexity
of natural language makes this substantially more
challenging than with our synthetic experiments,
and that the loss function and inductive biases of
current neural LMs are not well suited for doing so
without an infeasible amount of data.


(a) ˆp(x) given by text frequency model.

x y x y

10[2]

Sentence

10[1] length

4

10[0] 6

7

10 1 9

10

10 2 12

10[3] 10[5] 10[3] 10[5]

Training sentences Training sentences

(b) ˆp(x) given by trigram model.

Figure 3: Plot of gpˆ(x, y) = log _[p]p[ˆ]ˆ[(]([xy]x$)[)]_ _pˆ(y$)_ [as]

_[−]_ [log][ ˆ][p][(][yy][)]

a function of the number of sentences in the training
corpus and the length _xy_ . Given the true distribution
_|_ _|_
_p, gp(x, y) = 0 iff x entails y. We exclude pairs x, y_
where both xy and yy are absent from the training data.


-----

A natural next step for future work is to test
this hypothesis empirically by measuring whether
entailment judgments can be extracted from large
LMs using our theory. Similarly, it would be interesting to think about how LMs could be modified
so that they can better pick up on the semantic
information encoded in their training distribution.

### 10 Acknowledgments

This project benefited from informal discussions
with Chris Barker, Sam Bowman, Lucius Bynum,
Kyunghun Cho, Tiwa Eisape, Yanai Elazar, Najoung Kim, Alexander Koller, Vishakh Padmakumar, Chris Potts, Naomi Saphra, Sebastian Schuster, and Noah A. Smith. We also thank the members of the CAP Lab and Semantics Group at NYU
for their feedback. This work was supported in
part through the NYU IT High Performance Computing resources, services, and staff expertise. It
was funded by NSF award 1922658, and WM was
supported by an NSF graduate research fellowship.

### References

[Emily M. Bender and Alexander Koller. 2020. Climbing](https://doi.org/10.18653/v1/2020.acl-main.463)
[towards NLU: On meaning, form, and understanding](https://doi.org/10.18653/v1/2020.acl-main.463)
[in the age of data. In Proceedings of the 58th Annual](https://doi.org/10.18653/v1/2020.acl-main.463)
_Meeting of the Association for Computational Lin-_
_guistics, pages 5185–5198, Online. Association for_
Computational Linguistics.

Steven Bird. 2006. [NLTK: The Natural Language](https://doi.org/10.3115/1225403.1225421)
[Toolkit. In Proceedings of the COLING/ACL 2006](https://doi.org/10.3115/1225403.1225421)
_Interactive Presentation Sessions, pages 69–72, Syd-_
ney, Australia. Association for Computational Linguistics.

Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas
Card, Rodrigo Castellon, Niladri Chatterji, Annie
Chen, Kathleen Creel, Jared Quincy Davis, Dora
Demszky, Chris Donahue, Moussa Doumbouya,
Esin Durmus, Stefano Ermon, John Etchemendy,
Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor
Gale, Lauren Gillespie, Karan Goel, Noah Goodman,
Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny
Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil
Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth
Karamcheti, Geoff Keeling, Fereshte Khani, Omar
Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle
Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,
Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,


Avanika Narayan, Deepak Narayanan, Ben Newman,
Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,
Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob
Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,
Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa
Sadigh, Shiori Sagawa, Keshav Santhanam, Andy
Shih, Krishnan Srinivasan, Alex Tamkin, Rohan
Taori, Armin W. Thomas, Florian Tramèr, Rose E.
Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai
Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan
You, Matei Zaharia, Michael Zhang, Tianyi Zhang,
Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn
[Zhou, and Percy Liang. 2021. On the opportunities](https://doi.org/10.48550/ARXIV.2108.07258)
[and risks of foundation models.](https://doi.org/10.48550/ARXIV.2108.07258)

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
[Language models are few-shot learners.](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) In Ad_vances in Neural Information Processing Systems,_
volume 33, pages 1877–1901. Curran Associates,
Inc.

Judith Degen, Robert XD Hawkins, Caroline Graf, Elisa
Kreiss, and Noah D Goodman. 2019. When redundancy is rational: A Bayesian approach to “overinformative” referring expressions. arXiv preprint
_arXiv:1903.08237._

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
[Kristina Toutanova. 2019. BERT: Pre-training of](https://doi.org/10.18653/v1/N19-1423)
[deep bidirectional transformers for language under-](https://doi.org/10.18653/v1/N19-1423)
[standing. In Proceedings of the 2019 Conference of](https://doi.org/10.18653/v1/N19-1423)
_the North American Chapter of the Association for_
_Computational Linguistics: Human Language Tech-_
_nologies, Volume 1 (Long and Short Papers), pages_
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.

John R Firth. 1957. A synopsis of linguistic theory,
1930-1955. Studies in linguistic analysis.

Noah D Goodman and Michael C Frank. 2016. Pragmatic language interpretation as probabilistic inference. Trends in cognitive sciences, 20(11):818–829.

Noah D Goodman and Daniel Lassiter. 2015. Probabilistic semantics and pragmatics: Uncertainty in
language and thought. The handbook of contempo_rary semantic theory, 2nd edition. Wiley-Blackwell._

Herbert P Grice. 1975. Logic and conversation. In
_Speech acts, pages 41–58. Brill._

Irene Heim and Angelika Kratzer. 1998. Semantics in
_Generative Grammar. Blackwell._


-----

Irene Roswitha Heim. 1982. _The semantics of defi-_
_nite and indefinite noun phrases. University of Mas-_
sachusetts Amherst.

Hans A Kamp. 1981. A theory of truth and semantic
representation formal methods in the study of language, part 1, ed. by jeroen groenendijk, theo janssen
and martin and stokhof. Amsterdam: Mathematisch
_Centrum._

Angelika Kratzer. 2021. Situations in Natural Language
Semantics. In Edward N. Zalta, editor, The Stanford
_Encyclopedia of Philosophy, Winter 2021 edition._
Metaphysics Research Lab, Stanford University.

Stephen C Levinson, Stephen C Levinson, and S Levinson. 1983. Pragmatics. Cambridge university press.

David Lewis. 1979. Scorekeeping in a language game.
In Semantics from different points of view, pages 172–
187. Springer.

Shaojie Li and Yong Liu. 2021. Towards sharper generalization bounds for structured prediction. Advances
_in Neural Information Processing Systems, 34._

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
[TruthfulQA: Measuring how models mimic human](https://doi.org/10.18653/v1/2022.acl-long.229)
[falsehoods. In Proceedings of the 60th Annual Meet-](https://doi.org/10.18653/v1/2022.acl-long.229)
_ing of the Association for Computational Linguistics_
_(Volume 1: Long Papers), pages 3214–3252, Dublin,_
Ireland. Association for Computational Linguistics.

William Merrill, Yoav Goldberg, Roy Schwartz, and
[Noah A. Smith. 2021. Provable Limitations of Ac-](https://doi.org/10.1162/tacl_a_00412)
[quiring Meaning from Ungrounded Form: What Will](https://doi.org/10.1162/tacl_a_00412)
[Future Language Models Understand? Transactions](https://doi.org/10.1162/tacl_a_00412)
_of the Association for Computational Linguistics,_
9:1047–1060.

[Julian Michael. 2020. To dissect an octopus: Making](https://blog.julianmichael.org/2020/07/23/to-dissect-an-octopus.html)
[sense of the form/meaning debate.](https://blog.julianmichael.org/2020/07/23/to-dissect-an-octopus.html)

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
[Zettlemoyer. 2018. Deep contextualized word repre-](https://doi.org/10.18653/v1/N18-1202)
[sentations. In Proceedings of the 2018 Conference of](https://doi.org/10.18653/v1/N18-1202)
_the North American Chapter of the Association for_
_Computational Linguistics: Human Language Tech-_
_nologies, Volume 1 (Long Papers), pages 2227–2237,_
New Orleans, Louisiana. Association for Computational Linguistics.

[Christopher Potts. 2020. Is it possible for language](https://chrisgpotts.medium.com/is-it-possible-for-language-models-to-achieve-language-understanding-81df45082ee2)
[models to achieve understanding?](https://chrisgpotts.medium.com/is-it-possible-for-language-models-to-achieve-language-understanding-81df45082ee2)

Shaojun Wang, Russell Greiner, and Shaomin Wang.
2013. Consistency and generalization bounds for
maximum entropy density estimation. _Entropy,_
15(12):5439–5463.


-----

### A Limitations

We derived a recipe for computing entailment in terms of text probabilities, hinting that entailment
judgments may be decodable from LM predictions. Yet two key concerns qualify this conclusion.

**Learnability** We reduce entailment classification to computing probabilities in the target distribution of
an LM, not probabilities predicted by an LM. In §6, we argue that the loss function of current LMs is not
well suited to producing models from which entailment can be extracted.

**Speaker Assumptions** Gricean speakers capture important factors influencing speech production in
pragmatic theory, but human speakers are undoubtedly more complex. Based on §8, we expect a similar
isomorphism to hold under any reasonable speaker model, but the mathematical form may change and it
may become harder to compute.

### B Uncountable World Spaces

In this section, we assume is an uncountably infinite set with a a probability density function p(w).
_W_
We then define “almost sure” entailment as follows:

**Definition 7 For x, y**, we say x almost surely entails y (i.e., _x_ _y_ ) if and only if
_∈X_ � � _⊑_ � �

_p(_ _x_ _y_ ) = 0.
� � _\ �_ �

Note that if is countable, then reduces to . We can generalize Lemma 1 as follows,
_W_ _A ⊑B_ _A ⊆B_
which shows that all our results go through for almost sure entailment when is uncountable.
_W_

**Lemma 4 Let 1S be the indicator function for set S. Let f : W →** R be some function such that
inf _w∈W f_ (w) > 0. For any sets A, B such that A ⊆B ⊆W, then p(B \ A) = 0 if and only if

E E
_w∼p(w)_ [[][1][A][(][w][)][f] [(][w][)] =] _w∼p(w)_ [[][1][B][(][w][)][f] [(][w][)]][ .]

_Proof. If p(_ ) = 0, then the condition follows by construction. We thus only need to show that the
_B \ A_
condition follows from p( ) = 0. Let q = p( ). By linearity of expectation, we rewrite the
_B \ A_ _B \ A_
premise condition as

0 = E
_w∼p(w)_ [[(][1][A][(][w][)][ −] [1][B][(][w][))][ f] [(][w][)]]

= E
_w∼p(w)_ [[(][1][A][(][w][)][ −] [1][B][(][w][))][ f] [(][w][)][ |][ w][ ∈B \ A][]][ q]

+ E
_w∼p(w)_ [[(][1][A][(][w][)][ −] [1][B][(][w][))][ f] [(][w][)][ |][ w][ ̸∈B \ A][] (1][ −] _[q][)]_

_≥_ E
_w∼p(w)_ [[][f] [(][w][)][ |][ w][ ∈B \ A][]][ q.]

Letting f _[∗]_ = inf _w∈W f_ (w) > 0, we get 0 ≥ _f_ _[∗]q. Since f_ _[∗]_ _> 0 and q ≥_ 0, q = p(B \ A) = 0.

### C Gricean Speaker Proofs

**Lemma 2 For any Gricean speaker p and x** _,_
_∈X_

_p(x$)_
_p(xx) [= exp(]exp([c]c[(]($))[x][))]_ _[.]_

_Proof. Starting with the definition of an Gricean speaker, for any x ∈X_ _[∗]_ and y ∈X ∪{$},

_p(xy) = E_
_w_ [[][p][(][x][ |][ w][)][p][(][y][ |][ x, w][)]][ .]

Now, letting g(x, w) ≜ _p(x | w)/_ ��y[′][ exp (][αI][ℓ][(][y][′][ |][ x][;][ w][)][ −] _[c][(][y][′][))]�,_

_p(xy) = exp(−c(y)) Ew_ [[exp(][αI][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)]][ .]


-----

We apply this identity to both sides of the fraction in the lemma statement:

_p(x$)_
_p(xx) [= exp(]exp(−[−]c[c]([($))]x)) E[ E]w[w] [exp([ [exp(]αI[αI]ℓ[ℓ]([($]x[ |] |[ x] x[;];[ w] w[))]))[g]g[(]([x, w]x, w[)]])]_

= [exp(][c][(][x][))]

exp(c($)) _[·][ E]Ew[w] [exp([ [exp(]αI[αI]ℓ[ℓ]([($]x[ |] |[ x] x[;];[ w] w[))]))[g]g[(]([x, w]x, w[)]])]_ _[.]_


Since _x_ $ and _x_ _x_, we know that the conditional information of both $ and x given x is 0,
� � _⊆_ � � � � _⊆_ � �
and, thus,
_p(x$)_
_p(xx) [= exp(]exp([c]c[(]($))[x][))]_ _[·][ E]Ew[w] [exp(0)[ [exp(0)]g[g]([(]x, w[x, w])] [)]]_ [= exp(]exp([c]c[(]($))[x][))] _[.]_

**Theorem 2 Under any Gricean speaker p, entailment is isomorphic to a distributional relation. Specifi-**
_cally, for all sentences x, y_ _,_
_∈X_


_x_ _y_
� � _⊆_ � � _⇐⇒_ _[p]p[(]([xy]x$) [)]_ [=][ p]p[(]([yy]y$)[)] _[.]_

_Proof. Recall from the proof of Lemma 2 that there exists a function g(x, w) such that, for all x ∈X_ _[∗]_

and y $,
_∈X ∪{_ _}_
_p(xy) ∝_ exp(−c(y)) Ew [[exp(][αI][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)]][ .]


Thus, by Lemma 2, the proposed distributional relation can be expanded as

_dp(x, y) ⇐⇒_ _[p][(][xy][)]_

_p(x$) [=][ p]p[(]([yy]y$)[)]_


_p(xy)_ _[p][(][y][$)]_
_⇐⇒_ _·_

_p(yy) [=][ p][(][x][$)][ ·][ p]p[(]([xx]xx[)])_


_p(xy)_ [exp(][c][(][y][))]
_⇐⇒_

exp(c($)) [=][ p][(][xx][)exp(]exp([c]c[(]($))[x][))]

_p(xy) exp(c(y)) = p(xx) exp(c(x))_
_⇐⇒_

_⇐⇒_ Ew [[exp(][αI][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)] =][ E]w [[exp(][αI][ℓ][(][x][ |][ x][;][ w][))][g][(][x, w][)]][ .]

By Lemma 1 (Here is the error: Lemma 1 does not apply! See §H), this holds if and only if, for all w,

exp(αIℓ(y | x; w)) = exp(αIℓ(x | x; w))

_Iℓ(y | x; w) = Iℓ(x | x; w)_

_Iℓ(y | x; w) = 0_

_x_ (w) _y_ (w) = 1.
� � _→_ � �

We conclude the distributional relation holds if and only if _x_ _y_ .
� � _⊆_ � �

### D Proofs for Learning Bounds

**Lemma 3 For z ∈X** _[∗]_ _∪X_ _[∗]$ and δ > 0, it holds with probability at least 1 −_ _δ −_ (1 − _p(z))[n]_ _that_


�


Kp(z)

_δn [.]_


log p(z) log ˆp(z)
_|_ _−_ _| ≤_


_Proof. Without loss of generality, assume p(z) > 0. With probabiliy 1_ (1 _p(z))[n]_ over the draw of
_−_ _−_
our sample, the random variable log ˆp(z) has finite variance defined by

Var [log ˆp] = [1] _._

_≤_ [K][p][(][z][)]

_n_ _p(z)_ _n_

_[·][ 1][ −]_ _[p][(][z][)]_


-----

With finite variance, we can apply Chebyshev’s inequality to conclude that

Pr [ log p(z) log ˆp(z) _ϵ]_
_|_ _−_ _| ≥_ _≤_ [Var [log ˆ][p][]] _≤_ [K][p][(][z][)]

_ϵ[2]_ _nϵ[2][ .]_


Solving for δ Pr [ log p(z) log ˆp(z) ], we get
_≤_ _|_ _−_ _|_

_δ_
_≤_ [K][p][(][z][)]

_nϵ[2]_

�

Kp(z)

∴ _ϵ ≤_

_δn [.]_


We conclude that that with probability 1 _δ_ (1 _p(z))[n],_
_−_ _−_ _−_

�

log p(z) log ˆp(z)
_|_ _−_ _| ≤_


Kp(z)

_δn [.]_


We now characterize the complexity factor Kp(z) for uniformly truthful speakers.

**Lemma 5 For all z ∈X** _[∗]_ _∪X_ _[∗]$ such that �z�(p) > 0, it holds that_

_|X|_
Kp(z) ≤

_p(_ _z_ ) _[.]_
� �

_Proof. We start by deriving a lower bound on p(z)._


�
_p(z) =_

_w_

�
_≥_

_w_


_z_ (w)
� �
�
_z[′][�][z][′][�][(][w][)]_ _[p][(][w][)]_

_z_ (w)
� � _p(w)_

_|X|_


= [�][z][�][(][p][)] _[.]_

_|X|_

Applying this inequality to the definition of Kp(z), we conclude that


_|X|_
Kp(z) ≤

_z_ (p) _[.]_
� �

Lemma 5 lets us to derive the following guarantee for estimating entailment scores using a corpus
produced by uniformly truthful speakers:

**Theorem 4 For a uniformly truthful speaker p, let up(x, y) = log p(x$) −** log p(xy). For x, y ∈X such
_that_ _xy_ (p) > 0 and δ > 0, it holds with probability at least 1 _δ_ 2(1 _p(xy))[n]_ _that_
� � _−_ _−_ _−_


�


_|X|_

_p(_ _xy_ ) _δn_ _[.]_
� � _[·][ 2]_


_|up(x, y) −_ _upˆ(x, y)| ≤_ 2

_Proof. We expand the difference in scores as follows:_


_|up(x, y) −_ _upˆ(x, y)| ≤|log p(x) −_ log ˆp(x$)| + |log p(xy) − log ˆp(xy)|.


-----

We then apply Lemma 3 with _[δ]_

2 [. Since][ p][(][x][$)][ ≥] _[p][(][xy][)][, this implies that with probability][ 1][ −]_ _[δ][ −]_ [2(1][ −]
_p(xy))[n],_

� �

2Kp(x$) 2Kp(xy)

_|up(x, y) −_ _upˆ(x, y)| ≤_ +

_δn_ _δn_

�

2 max{Kp(x$), Kp(xy)}

2 _._
_≤_

_δn_

Finally, we apply Lemma 5 to conclude that


�

_|up(x, y) −_ _upˆ(x, y)| ≤_ 2

�


_|X|_

min _x$_ (p), _xy_ (p) _δn_
_{�_ � � � _} [·][ 2]_

_|X|_

_xy_ (p) _δn_ _[.]_
� � _[·][ 2]_


= 2


We now characterize the complexity factor for Gricean speakers.

**Lemma 6 Assume that p(z** _w) is a Gricean speaker with respect to listener ℓ_ _and_ _z_ (w) = 1
_|_ � � _⇐⇒_
_Iℓ(z; w) ≥_ 0. Then, for all z ∈X _[∗]_ _∪X_ _[∗]$,_

Kp(z) ≤ [exp(][c][(][z][))] _._

_p(_ _z_ )
� �

_Proof. We start by writing out the form of p(z):_


�
_w_ [exp(][αI][ℓ][(][z][;][ w][))][p][(][w][)]
_p(z) =_ _._

exp(c(z))

Because z ∈X _[∗]_ _∪X_ _[∗]$, all terms where �z�(w) = 1 contribute at least 0 information; other terms_
contribute negative information. Thus, we bound the information content of the “true” terms above 0, and
ignore the other terms to get the lower bound

�
_w[�][z][�][(][w][) exp(0)][p][(][w][)]_
_p(z)_
_≥_

exp(c(z))

�
_w[�][z][�][(][w][)][p][(][w][)]_
=

exp(c(z))


_z_ (p)
= � �

exp(c(z)) _[.]_

Plugging this into Kp(z), we conclude that

Kp(z) ≤ [exp(][c][(][z][))] _._

_z_ (p)
� �


**Theorem 3 Assume that p(z** _w) is a Gricean speaker with respect to listener ℓ_ _and_ _z_ (w) = 1
_|_ � � _⇐⇒_
_Iℓ(z; w) ≥_ 0. Let gp(x, y) = log _[p]p[(]([xy]x$)[)]_ _p(y$)_ _[. Let][ q][ = 1][ −]_ [min][{][p][(][xy][)][, p][(][yy][)][}][. Then, for all]

_[−]_ [log][ p][(][yy][)]

_x, y_ _such that_ _xy_ (p) > 0, for all δ > 0, it holds with probability at least 1 _δ_ 4q[n] _that_
_∈X_ � � _−_ _−_
_|gp(x, y) −_ _gpˆ(x, y)| is at most_


�


exp(max _c(xy), c(yy)_ )
_{_ _}_

[1]

_·_
_p(_ _xy_ ) _δn_ _[.]_
� �


8


-----

_Proof. We start by expanding gp(x, y):_

_gp(x, y) = log_ _[p][(][xy][)]_

_p(x$)_ _p(y$)_

_[−]_ [log][ p][(][yy][)]

= log p(xy) log p(x$) log p(yy) + log p(y$).
_−_ _−_


Thus, following Theorem 4, we can bound

_|gp(x, y) −_ _gpˆ(x, y)| ≤|log p(xy) −_ log ˆp(xy)| + |log p(x$) − log ˆp(x$)|

+ log p(yy) log ˆp(yy) + log p(y$) log ˆp(y$) _._
_|_ _−_ _|_ _|_ _−_ _|_

We apply Lemma 3 to each term with _[δ]_

4 [. Since][ p][(][yy][)][ ≤] _[p][(][y][$)][ and][ p][(][xy][)][ ≤]_ _[p][(][x][$)][, we get that with]_
probability at least 1 _δ_ 4q[n],
_−_ _−_


�

_|gp(x, y) −_ _gpˆ(x, y)| ≤_ 4

�

= 8


4 max{Kp(xy), Kp(x$), Kp(yy), Kp(y$)}

_δn_

max{Kp(xy), Kp(x$), Kp(yy), Kp(y$)}

_._
_δn_


Finally, we apply Lemma 6 to conclude that, with probability at least 1 _δ_ 4q[n],
_−_ _−_


_|gp(x, y) −_ _gpˆ(x, y)| ≤_ 8

8
_≤_


�

� exp(c(xy))
max _,_ [exp(][c][(][x][$))] _,_ [exp(][c][(][yy][))] _,_ [exp(][c][(][y][$))]

_xy_ (p) _x$_ (p) _yy_ (p) _y$_ (p)
� � � � � � � �

�

exp(max _c(xy), c(yy)_ )
_{_ _}_

[1]

_·_
_xy_ (p) _δn_ _[.]_
� �


� [1]

_·_

_δn_


We can use Corollary 2.1 to derive a tighter version of Theorem 3 by removing the dependence on the
uncommon string yy:

**Theorem 5 Let sp(x, y) = log** _p[p]([(]xy[x][$)])_ _[−]_ _[c][(][y][) +][ c][($)][ . Then, for all][ x, y][ ∈X][ such that][ �][xy][�][(][p][)][ >][ 0][, for]_

_all δ > 0, the following holds with probability 1_ _δ_ 2(1 _p(xy))[n],_
_−_ _−_ _−_


�

_|sp(x, y) −_ _spˆ(x, y)| ≤_ 2


exp(c(xy)) [2]

_·_
_p(_ _xy_ ) _δn_ _[.]_
� �


The proof follows analogously to Theorem 3. The main improvement of Theorem 5 compared to
Theorem 3 is that the probability the bound holds no longer depends on the unlikely probability p(yy).
We also get the benefit that the cost complexity factor has been reduced to only depend on c(xy) and
_√_
obtain better constants (2 2 instead of 8), although these changes are likely less important than removing

the dependence on p(yy). Of course, the drawback is that we are assuming access to the cost function
_c(y). If we have such access, though, the improvements in the bound suggest we may be able to extract_
entailment from a finite corpus of Gricean text with better sample complexity than if we did not.

### E Sample Complexity Estimation Details

Assuming the approximation error in Theorem 3 is _ϵ, we aim to solve the following inequality for n:_
_≤_


�


exp(max _c(xy), c(yy)_ )
_{_ _}_

[1]

_·_
_p(_ _xy_ ) _δn_ _[.]_
� �


_ϵ_ 8
_≤_


-----

**Sentence Length** We make the simplifying assumption that max _c(xy), c(yy)_ = 2w(ℓ + 1), where ℓ
_{_ _}_
is a variable representing sentence length.[16] Let Σ be the word-level vocabulary of English. We estimate
the value w by assuming q(z) = exp(−w(|z| + 1)) is a valid prior over Σ[∗] and solving for the unique
value of w to satisfy this condition:

�

exp( _w(_ _z_ + 1)) = 1
_−_ _|_ _|_
_z∈Σ[∗]_

�∞ _|Σ|[ℓ]_

exp(w(ℓ + 1)) [= 1]

_ℓ=0_


�ℓ
= 1


� Σ
_|_ _|_

exp(w)


exp( _w)_
_−_


_∞_
�

_ℓ=0_


∴ _w = log(|Σ| + 1)._

This reveals that w should be set 1, but the question remains how to set Σ . In practice, we assume the
_≥_ _|_ _|_
speaker prior is defined over the support of all syntactically valid or likely strings in English, not over all
possible strings as derived above. Letting S be the word-level perplexity of English, we set w according to

_w_ log(S + 1).
_≈_

We set S to the value estimated by GPT-3: 20 nats/word (Brown et al., 2020). Simplifying the numerator
_∼_
in the bound yields

exp(log(21)(ℓ + 1)) = 21[ℓ][+1].

Making the prior less strong, i.e., increasing Σ to be greater than this perplexity estimate, would only
_|_ _|_
increase the number of samples needed to extract entailment judgments.

**Truth Probability** We conservatively assume p(�xy�) = 2[1] [, although in practice it may be smaller for]

more informative sentences. Reducing it would lead to higher sample complexity estimates.

**Final Form** Putting together our estimates for sentence length and truth probability yields


�

2 21[ℓ][+1]
_·_

_ϵ_ 8
_≤_

_δn_

∴ _n ≤_ [128][ ·][ 21][ℓ][+1] _._

_δϵ[2]_


The final form captures the intuition that the likelihood of a string vanishes exponentially with its length,
and that the base of this decay is roughly inversely proportional to the perplexity of the language. In
practice, we set δ = 0.1 and ϵ = 1.0. Changing the value of ϵ (the desired approximation accuracy) would
shift the curve.

### F General Relations and Speakers

So far, we have characterized concrete distributional relations that are isomorphic to entailment for
different classes of speaker models. In this section, we analyze the conditions under which a distribution
relation isomorphic to a semantic relation exists, given no assumptions about the speaker. Informally, we
prove in Theorem 6 that a distributional isomorphism exists if and only if the speaker model depends
on semantics “at all”. This is a very weak condition, and should be satisfied by any reasonable model
of natural speakers. Thus, we take this as evidence that any speaker model—not just the ones we have
considered, admits a distributional relation isomorphic to entailment.

16We write ℓ + 1 instead of ℓ here for technical reasons: we want to guarantee that q(z) can be a valid probability distribution.


-----

We now turn to the formal presentation of this result. Let M be the function that takes a set of worlds
_W and returns all semantic evaluation functions µ : X �→_ 2[W] over W. For a semantic evaluation function
_µ = λx._ _x_, let pµ be a speaker model parameterized by semantics µ.
� �
Say two semantic evaluation functions µ, µ[′] are isomorphic with respect to s if and only if, for all x, y,

_S(µ(x), µ(y))_ _S(µ[′](x), µ[′](y))._
_⇐⇒_

**Theorem 6 The following are equivalent for any speaker p and semantic relation s:**

_1. There exists a distribution relation d such that, for all W, for all µ ∈_ _M_ (W), s is isomorphic to dpµ.

_2. For all_ _,_ _, for all µ_ _M_ ( ) and µ[′] _M_ ( ) such that µ, µ[′] _are not isomorphic w.r.t. s, there_
_W_ _W_ _[′]_ _∈_ _W_ _∈_ _W_ _[′]_
_exists z ∈X_ _[∗]_ _such that pµ(z) ̸= pµ′(z)._

_Proof. We will show that equivalence holds in both directions._

Forward Direction: We assume the second statement does not hold by way of modus tollens. Thus,
there exists W, W _[′]_ with µ ∈ _M_ (W) and µ[′] _∈_ _M_ (W _[′]) with µ, µ[′]_ not isomorphic such that, for all z ∈X _[∗],_
_pµ(z) = pµ′(z). Thus, for all d and sentences x, y,_

_dpµ(x, y) ⇐⇒_ _dpµ′_ (x, y).

But µ and µ[′] are not isomorphic, so there exist x, y such that S(µ(x), µ(y)) _S(µ[′](x), µ[′](y)). Thus,_
_̸⇐⇒_
we can conclude that one of the following must hold:

_dpµ(x, y)_ _̸⇐⇒_ _S(µ(x), µ(y))_

_dpµ′_ (x, y) _̸⇐⇒_ _S(µ[′](x), µ[′](y))._

We conclude by modus tollens that the first statement implies the second.

Backward Direction: Assume the second statement holds. The function f (µ) = pµ is invertible up to
isomorphism to s. In other words, there exists g(pµ) = µ[∗] such that, for all x, y,

_S(µ[∗](x), µ[∗](y))_ _S(µ(x), µ(y))._
_⇐⇒_

Then we define d according to

_dpµ(x, y) ⇐⇒_ _S(g(pµ)(x), g(pµ)(y))_

_S(µ[∗](x), µ[∗](y))_
_⇐⇒_

_S(µ(x), µ(y))._
_⇐⇒_

Thus, the second statement implies the first.

### G Experimental Details

**G.1** **Language Description**

We set the vocabulary = 100, 010, 001, 110, 011, 111 and define = 1, 2, 3 . We refer to
_X_ _{_ _}_ _W_ _{_ _}_
each three-digit binary string as an utterance, and define the evaluation function for an utterance x as
�x�(w) = 1 ⇐⇒ _xw = 1. Thus, 100 is true only in world 1, while 111 is true in all worlds (i.e., is_
tautological). We identify 111 with the end of sequence $.
In line with our formal definitions, we define a text z as a concatenation of utterances z1 _zn ending_
_· · ·_
with $. Recall that we define the evaluation function over a text as the intersection of the evaluation
functions of the utterances it contains. For our language, this reduces to �z�(w) = 1 ⇐⇒∀i (zi = 1).
Thus, 011 101 111 is true only in w3, and 011 101 110 111 is true in no worlds (i.e., contradictory).


-----

Figure 4: Properties of the data generated by the speaker in our experiments, with α = 5 and c(x) = 0.1 _x_ .
_· |_ _|_

**G.2** **Speaker Model Parameters**

We model the listener of the informative speaker as a literal listener (Goodman and Frank, 2016), which
means our informative speaker is a rational speaker of depth 1 in the language of rational speech acts.
We set c(x) = 0.1 _x_, where _x_ is the length of the string x. We set the rationality parameter
_· |_ _|_ _|_ _|_
_α = 5. These choices were made heuristically, by inspecting the the properties of the speaker’s output,_
as summarized in Figure 4. These parameters led to a relatively uniform distribution over utterances
(except for the stop token 111 which is present in all texts), and a variety of text lengths without excessive
redundancy. We found that larger values of α or of the coefficient for the cost function produced short
texts, biasing maximally informative utterances (i.e., 100, 010, or 001); while smaller values produced
long, repetitive utterances or sometimes empty utterances.

**G.3** **Training and Evaluation**

We sample a dataset from a speaker by independently sampling n texts from the speaker model. We
generate datasets of varying size from each speaker, with the number of texts n decreasing by factors of 2
from 10[7] texts down to just 2 texts.
We train models of two kinds: a text frequency model, and a trigram model. The text frequency model
simply assigns a probability to a text proportional to its frequency in the training data, assigning a small
_ϵ = 10[−][20]_ probability to an unknown sequence. The trigram model is trained using NLTK’s (Bird, 2006)
MLE implementation, i.e., the probabilities are unsmoothed. We do not need to use smoothing due to the
small number of possible trigrams in the language.
For evaluation data, we generate pairs of texts labeled for entailments. We include all pairs where each
text is 6 utterances or shorter, except for utterances that are contradictory or consist only of the end of
sequence token. The total number of test pairs is about 1.1M.

### H Erratum Derivation

Formally, _x_ can be partitioned into two sets of worlds:
� �

1. Y = _x_ _y_ where x, y are both true
� � _∩_ � �
2. _Y[˜] =_ _x_ _y_ where x is true but y is false
� � _\ �_ �
Following the initial reasoning in the original proof of Theorem 2, the entailment test is 0 if and only if


E )g(x, w)]
_w[[exp(][αI][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)] =][ E]w[[exp(][α I][ℓ][(][x][ |][ x][;][ w][)]_
� =0�� �

E
_w[[exp(][αI][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)] =][ E]w[[][g][(][x, w][)]][.]_


-----

Flipping the equation for convenience, we get that

E
_w[[][g][(][x, w][)] =][ E]w[[exp(][αI][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)][ |][ w][ ∈]_ [�][x][�][]]

= p(Y ) Ew[[][α][ exp(][I][ℓ][(][y][ |][ x][;][ w][))][g][(][x, w][)][ |][ w][ ∈] _[Y][ ] +]_ hhhhhhhhhhhhhhhhhp( Y[˜] ) Ew∈Y[˜] [exp(�Iℓ(y= |��−∞ x; w�)) | w ∈ _Y[˜] ]._


Now, define

_I(Y ) ≜_ E
_w[[exp(][I][ℓ][(][y][ |][ x][;][ w][))][ |][ w][ ∈]_ _[Y][ ]][,]_

_I ≜_ E
_w[[][g][(][x, w][)]][.]_

We conclude that the entailment test is 0 if and only if p(Y )I(Y ) = I.
Note that both I(Y ) and I implicitly depend on x. They can be interpreted as measures of the
conditional information of y and the empty string after x, respectively.


-----

