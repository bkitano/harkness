## What Formal Languages Can Transformers Express? A Survey


### Lena Strobl UmeÃ¥ University, Sweden
 lena.strobl@umu.se


### William Merrill New York University, USA
 willm@nyu.edu


### Gail Weiss EPFL, Switzerland
 gail.weiss@epfl.ch


### David Chiang University of Notre Dame, USA
 dchiang@nd.edu

 Abstract


### Dana Angluin Yale University, USA
 dana.angluin@yale.edu

of formal languages â€“ that is, the inputs or outputs are treated as sequences of discrete symbols
from a finite alphabet, and crucially as sequences
of unbounded length.
The core research question in this subarea is:
_How can we characterize the expressivity of trans-_
_formers in relation to various formal models, such_
_as automata, boolean circuits or formal logic? Ap-_
plications of this subarea, which are not addressed
by the papers surveyed here but could be by future
work, would hopefully answer questions like:


As transformers have gained prominence
in natural language processing, some researchers have investigated theoretically
what problems they can and cannot solve,
by treating problems as formal languages.
Exploring such questions can help clarify
the power of transformers relative to other
models of computation, their fundamental
capabilities and limits, and the impact of architectural choices. Work in this subarea has
made considerable progress in recent years.
Here, we undertake a comprehensive survey
of this work, documenting the diverse assumptions that underlie different results and
providing a unified framework for harmonizing seemingly contradictory findings.

### 1 Introduction


Transformers (Vaswani et al., 2017) have gained
prominence in natural language processing (NLP),
both in direct applications like machine translation and in pretrained models like BERT (Devlin
et al., 2019) and GPT (Radford et al., 2018; Brown
et al., 2020; OpenAI, 2023). Consequently, some
researchers have sought to investigate their theoretical properties. Such studies can broadly be divided
into studies of expressivity and trainability. While
trainability is very important and the focus of much
study (e.g., Bhattamishra et al., 2023; Allen-Zhu
and Li, 2023), here we focus on expressivity, which
is a prerequisite for trainability.
Studies of expressivity could be further divided
into those from the perspectives of approximation
theory and of formal language theory. The former
(e.g., Yun et al., 2020; Sanford et al., 2023), investigates transformers as approximators of various
classes of functions, along the lines of the universal
approximation theorem for feedforward neural networks (Hornik et al., 1989; Cybenko, 1989). The
latter, which is the subject of this survey, investigates transformers as recognizers or generators



  - What new transformer variants are suggested
by formal models?

  - Do failure cases anticipated from formal models occur in practice?

  - What insights into the complexity of human
language are offered by a characterization of
transformer expressivity?

This paper provides a comprehensive survey of
research in this subarea. Compared to the surveys of Ackerman and Cybenko (2020) and Merrill
(2021, 2023), which cover convolutional neural networks (CNNs), RNNs, and transformers, this is a
narrower, but deeper, survey on transformers only.
Interpreting theoretical transformer results is
complex due to diverse assumptions. Many variants of transformers exist in practice, and even
more have been proposed in theory. This diversity
leads to varied, even seemingly contradictory, results. We set up a unified framework for talking
about transformer variants (Â§4), and discuss how
some of these variants compare to one another in
expressivity.
We then provide background on various formal
models that transformers have been compared with
(Â§5). Then, in Â§6, we systematically survey current results in this literature, documenting their
assumptions and claims in terms of the definitions
of Sections 4 and 5.


1


-----

### 2 Overview

Table 1 summarizes the results surveyed here. One
way to classify them is into lower bounds (what
transformers can do) and upper bounds (what transformers canâ€™t do).
Much work on lower bounds has looked at au_tomata like finite automata, counter machines, and_
Turing machines, all of which had been successfully related to RNNs before (Siegelmann and Sontag, 1995; Merrill, 2020). This wide diversity of
machines is due to different variants of transformers, especially whether a transformer decoder is allowed to take a number of intermediate steps before
outputting a decision (Â§4.3.4), which dramatically
increases its power (Â§6.1).
By contrast, investigation of upper bounds has
mainly focused on circuit complexity (Â§5.2), which
had been successfully related to feedforward networks before (Parberry, 1994; Siu et al., 1995; Beiu
and Taylor, 1996; Å Ã­ma and Orponen, 2003). This
line of research began with restricted models of
transformer encoders and progressed to increasingly realistic variants and tighter bounds. One
way to restrict transformers is by discretizing the
attention mechanism (Â§4.2.1); another is to limit
the precision of number representations (Â§4.4).
More recent work has turned to formal logic
(Â§5.3) as a way of characterizing the expressive
power of transformers. The finer control afforded
by logics opens the possibility for them to be used
as upper bounds, lower bounds, or both.

### 3 Preliminaries

**Sets** We denote by N0 = {0, 1, 2, . . .} and N =
N0\{0} the set of natural numbers with and without
0, respectively. We write _ğ‘›_ = 0, 1, 2, . . ., ğ‘› 1
[ ] { âˆ’ }
for any ğ‘› N. We write Î£ for a finite alphabet,
âˆˆ
which, in NLP applications, is the set of words or
subwords known to the model.

**Vectors** We use ğ‘‘, ğ‘‘[â€²], etc., for dimensionalities
of vector spaces, lowercase bold letters (x, y, . . . )
for vectors, and uppercase bold letters (X, Y, . . . )
for matrices. For any vector x R[ğ‘‘], we number its
âˆˆ
elements starting from 0. For ğ‘– âˆˆ[ğ‘‘], we write xğ‘–
or [x]ğ‘– (not ğ‘¥ğ‘–) for the ğ‘–-th component of x.

**Sequences** For any set ğ´, we write ğ´[âˆ—] for the set
of all finite sequences over ğ´. We write the length
of a sequence ğ‘  _ğ´[âˆ—]_ as _ğ‘ _ and number its elements
âˆˆ | |
starting from 0; thus, ğ‘  = ğ‘ 0ğ‘ 1 Â· Â· Â· ğ‘ |ğ‘  |âˆ’1. We use
the variable ğ‘¤ for a string in Î£[âˆ—] and ğ‘› for the length


of ğ‘¤. For sequences in R[âˆ—], we use lowercase bold
letters (x, y, . . .), and for sequences in R[ğ‘‘], we
( )[âˆ—]
use the variable ğ‘‹.
A function ğ‘“ : ğ´[âˆ—] _ğµ[âˆ—]_ is length-preserving if
â†’
_ğ‘“_ _ğ‘¤_ = _ğ‘¤_ for all ğ‘¤ _ğ´[âˆ—]. For every function_
| ( )| | | âˆˆ
_ğ‘”_ : ğ´ _ğµ, we denote its extension to sequences_
â†’
by ğ‘” as well. That is, ğ‘” : ğ´[âˆ—] _ğµ[âˆ—]_ is defined as
â†’
follows: for all ğ‘  âˆˆ _ğ´[âˆ—]_ and ğ‘– âˆˆ[|ğ‘ |], ğ‘”(ğ‘ )ğ‘– = ğ‘”(ğ‘ ğ‘–).

**Neural networks** An affine transformation is
a function ğ¿ : R[ğ‘‘][in] R[ğ‘‘][out] parameterized by
â†’
weights Wğ¿ âˆˆ R[ğ‘‘][out] [Ã—][ğ‘‘][in] and bias bğ¿ âˆˆ R[ğ‘‘][out] such
that for every x âˆˆ R[ğ‘‘][in], ğ¿(x) = Wğ¿x + bğ¿. We say
that ğ¿ is linear if bğ¿ = 0.
The activation functions we use are the recti_fied linear unit (ReLU)_ _ğ‘¥_ = max _ğ‘¥, 0_ and the
R( ) ( )
logistic sigmoid function ğœ _ğ‘¥_ = 1 1 _ğ‘’[âˆ’]_ _[ğ‘¥]_ .
( ) /( + )
The softmax function : R[âˆ—] R[âˆ—] converts any
S â†’
sequence of reals into a probability distribution:

_ğ‘’[x][ğ‘–]_
S(x)ğ‘– = ï¿½ âˆ€ğ‘– âˆˆ[|x|].

_ğ‘–_ âˆˆ[|x|] _[ğ‘’][x][ğ‘–]_

### 4 Transformers

In this section, we define transformers and relevant
variants, and how transformers are used to describe
formal languages. For additional background on
transformers (not in relation to formal languages),
Huang et al. (2022) give a lucid commentary on
the original paper, Phuong and Hutter (2022) give
formal definitions and pseudocode, and Lin et al.
(2022) survey many variants of transformers.
Transformers are composed of an input layer
(Â§4.1), one or more hidden layers (Â§4.2), and an
output layer (Â§4.3). The inputs and outputs of the
layers are sequences of vectors, which we treat as
members of R[ğ‘‘] .[1]
( )[âˆ—]

**4.1** **Input layer**

Strings are initially mapped to sequences of vectors using a length-preserving function ğ‘’ : Î£[âˆ—]
â†’
R[ğ‘‘], which is the sum of a word embedding
( )[âˆ—]
WE : Î£ R[ğ‘‘] and a position(al) embedding or
â†’
_encoding PEğ‘›_ : [ğ‘›] â†’ R[ğ‘‘] for ğ‘› âˆˆ N:

_ğ‘’(ğ‘¤0 Â· Â· Â· ğ‘¤ğ‘›âˆ’1)ğ‘–_ = WE(ğ‘¤ğ‘–) + PEğ‘› (ğ‘–).

In theoretical constructions, the word embedding
can be any computable function.

1This differs from the original paper (Vaswani et al., 2017),
which treats them as matrices in R[ğ‘›][Ã—][ğ‘‘]. Our notation aligns
better with notation for formal languages and emphasizes the
variability of the sequence length.


2


-----

The original transformer paper (Vaswani et al.,

2017) introduced the following position embedding:



[PEğ‘› (ğ‘–)] ğ‘— =


ï¿½
sin 10000[âˆ’] _[ğ‘—][/][ğ‘‘]_ _ğ‘–_ if ğ‘— even
(   - )
cos 10000[âˆ’(][ ğ‘—] [âˆ’][1][)/][ğ‘‘] _ğ‘–_ if ğ‘— odd.
(    - )


Theoretical papers have explored other position
embeddings, including ğ‘– itself (PÃ©rez et al., 2021),
_ğ‘–_ _ğ‘›_ (Yao et al., 2021; Chiang and Cholak, 2022),
/
and 1 _ğ‘–_ or 1 _ğ‘–[2]_ (PÃ©rez et al., 2021).
/ /

**4.2** **Hidden layers**

A transformer layer is a length-preserving function
: R[ğ‘‘] R[ğ‘‘] . There are two variants. The
L ( )[âˆ—] â†’( )[âˆ—]
_post-norm variant (Vaswani et al., 2017) is_

_ğ‘‹_ [â€²] = N1(ğ‘‹ + A(ğ‘‹))

L(ğ‘‹) = N2(ğ‘‹ [â€²] + F (ğ‘‹ [â€²])) (1)

and the pre-norm variant (Wang et al., 2019) is

_ğ‘‹_ [â€²] = ğ‘‹ + A(N1(ğ‘‹))

L(ğ‘‹) = ğ‘‹ [â€²] + F (N2(ğ‘‹ [â€²])) (2)

where

  - is a multi-head self-attention with ğ‘‘ inA
put/output dimensions, ğ» heads, and ğ‘‘kv
key/value dimensions per head

  - is a feed-forward network (Â§4.2.2) with ğ‘‘
F
input/output dimensions and ğ‘‘ff hidden dimensions

  - N1 and N2 are layernorms with ğ‘‘ dimensions.

We define each of these components below.

**4.2.1** **Attention**

Attention was initially developed to facilitate retrieval of previously processed data from a variablelength history (Bahdanau et al., 2015). Transformers use a simple variant of attention known as
_scaled dot-product attention._

**Scaled dot-product attention with ğ‘‘** input/output
dimensions and ğ‘‘kv key/value dimensions is a function A: R[ğ‘‘] R[ğ‘‘] R[ğ‘‘] parameterized by linear
Ã— ( )[âˆ—] â†’
transformations

_ğ‘ŠA[Q][, ğ‘Š]A[K][, ğ‘Š]A[V]_ [:][ R][ğ‘‘] [â†’] [R][ğ‘‘][kv] _ğ‘ŠA[O]_ [:][ R][ğ‘‘][kv][ â†’] [R][ğ‘‘]


and defined for every z R[ğ‘‘], ğ‘‹ R[ğ‘‘] (with
âˆˆ âˆˆ( )[âˆ—]
_ğ‘‹_ = ğ‘›), and ğ‘— _ğ‘›_ as
| | âˆˆ[ ]

_ğ‘Š_ [Q]
**s(z, ğ‘‹) ğ‘—** = A [(][z][) Â·]âˆš _[ ğ‘Š]A[K][(][ğ‘‹]_ _[ğ‘—][)]_ (3)

_ğ‘‘kv_

_ğ›¼_ **z, ğ‘‹** = **s** **z, ğ‘‹** (4)
( ) S( ( ))

ï¿½âˆ‘ï¸ ï¿½
A(z, ğ‘‹) = ğ‘ŠA[O] _ğ›¼(z, ğ‘‹) ğ‘—_ _ğ‘ŠA[V][(][ğ‘‹]_ _[ğ‘—][)]_ _._

_ğ‘—_ âˆˆ[ğ‘›]


Typically, A is extended to a function A: R[ğ‘‘]
( )[âˆ—] Ã—
R[ğ‘‘] R[ğ‘‘] that is length-preserving in its first
( )[âˆ—] â†’( )[âˆ—]
argument. In cross-attention, z is computed by the
decoder while ğ‘‹ is computed by the encoder. In
_self_ -attention, the two arguments are identical:

SA: R[ğ‘‘] âˆ— Rğ‘‘ âˆ—
( ) â†’( )

SA _ğ‘‹_ = A _ğ‘‹, ğ‘‹_ _._
( ) ( )

**Attention masking** In future masked (also
known as causally masked) self attention, a term
_ğ‘š_ _ğ‘–, ğ‘—_ is added to Eq. (3) to force every position
( )
to attend only to preceding positions:


_ğ‘š_ _ğ‘–, ğ‘—_ =
( )


ï¿½
0 if ğ‘— _ğ‘–_
â‰¤
otherwise.
âˆ’âˆ


Some papers use strict future masking, that is,
_ğ‘š_ _ğ‘–, ğ‘—_ = 0 iff ğ‘—< ğ‘–, and occasionally past mask( )
ing ( _ğ‘—_ _ğ‘–) and strict past masking (_ _ğ‘—> ğ‘–)._
â‰¥

**Multi-head attention with ğ‘‘kv key/value dimen-**
sions per head is the sum of ğ» attentions with ğ‘‘kv
key/value dimensions:

âˆ‘ï¸
A(z, ğ‘‹) = Aâ„ (z, ğ‘‹).

_â„âˆˆ[_ _ğ»_ ]

Multi-head self attention is defined analogously.
This is equivalent to the original formulation,
which concatenated the outputs of the heads and
passed the result through a shared, larger, ğ‘Š [O]
A [.]

**Hard attention** Some theoretical analyses simplify attention by replacing the softmax with variants that focus attention only on the position(s)
with the maximum value, breaking ties in various
ways. For any s R[âˆ—], let ğ‘€ **s** = _ğ‘–_ **s** _ğ‘—_
âˆˆ ( ) { âˆˆ[| |] | âˆ€ âˆˆ

[|s|], s _ğ‘—_ â‰¤ **sğ‘–** } be the set of indices of the maximal
elements of s. In leftmost-argmax, the leftmost
maximal element is used:

[Sh(s)]ğ‘– = I[ğ‘– = min ğ‘€ (s)]


3


-----

whereas in average-argmax the maximal elements
share weight equally:

[Sa(s)]ğ‘– = [I][[][ğ‘–] [âˆˆ] _[ğ‘€]_ [(][s][)]] _._

_ğ‘€_ **s**
| ( )|

If softmax is thought of as a Boltzmann distribution,
then average-argmax is its low-temperature limit.
By substituting Sh or Sa for S in Eq. (4), we
get leftmost-hard and average-hard attention, respectively. Leftmost-hard attention was previously
called hard attention by Hahn (2020) and unique
_hard attention by Hao et al. (2022). One may also_
consider rightmost-hard attention, in which the
rightmost maximal element is used. Average-hard
attention was also called hard attention by PÃ©rez
et al. (2021) and saturated attention by Merrill
et al. (2022), and has been argued to be a realistic
approximation to how trained transformers behave
in practice (Merrill et al., 2021).

**4.2.2** **Feed-forward networks**
Although feed-forward networks can take many
forms, in the context of transformers, we use the following definition. A feed-forward network (FFN)
with ğ‘‘ input/output dimensions and ğ‘‘ff hidden dimensions is a function : R[ğ‘‘] R[ğ‘‘] parameterF â†’
ized by two affine transformations, ğ¿[1]F [:][ R][ğ‘‘] [â†’] [R][ğ‘‘][ff]

and ğ¿[2]F [:][ R][ğ‘‘][ff][ â†’] [R][ğ‘‘][, such that]

**x** = ğ¿[2]
F ( ) F [(R(][ğ¿][1]F [(][x][)))]

where is applied component-wise.
R

**4.2.3** **Layer normalization**
A ğ‘‘-dimensional layer normalization (Ba et al.,
2016), or layernorm for short, is a function
: R[ğ‘‘] R[ğ‘‘] parameterized by vectors ğ›¾ _, ğ›½_
N â†’ N N âˆˆ
R[ğ‘‘] and scalar ğœ€ 0:
N â‰¥

**x** **xÂ¯**
âˆ’
**x** = ğ›¾ _ğ›½_
N ( ) N âŠ™ âˆšï¸ + N

var **x** _ğœ€_
( ) + N

where is component-wise multiplication and
âŠ™


**xÂ¯ =** [1]

_ğ‘‘_


âˆ‘ï¸

**xğ‘–** var(x) = [1]

_ğ‘‘_

_ğ‘–_ âˆˆ[ğ‘‘ ]


âˆ‘ï¸

(xğ‘– âˆ’ **xÂ¯)[2].**
_ğ‘–_ âˆˆ[ğ‘‘ ]


The original definition of layernorm (Ba et al.,

2016) sets ğœ€ N = 0, but, for numerical stability,
all implementations we are aware of set ğœ€ N > 0.
Observe that N is Lipschitz-continuous iff ğœ€ N > 0.
Some transformer analyses omit for simplicN
ity (PÃ©rez et al., 2021), while others set ğœ€ N to
achieve various effects (Hahn, 2020; Chiang and
Cholak, 2022).


**4.3** **Networks and output layers**

We now define a complete transformer network.

**4.3.1** **Transformer encoders**
A transformer encoder is a length-preserving function : Î£[âˆ—] R[ğ‘‘] parameterized by the weights
T â†’( )[âˆ—]
of an input layer ğ‘’ and ğ· transformer layers
L1, . . ., Lğ·. A post-norm transformer encoder is:

T (ğ‘¤) = Lğ· â—¦Â· Â· Â· â—¦L2 â—¦L1 â—¦ _ğ‘’(ğ‘¤)_

where each Lğ‘™ is a post-norm layer (1) and â—¦ is
function composition. A pre-norm transformer encoder is additionally parameterized by the weights
of a final layernorm and is defined as:
N

T (ğ‘¤) = N â—¦Lğ· â—¦Â· Â· Â· â—¦L2 â—¦L1 â—¦ _ğ‘’(ğ‘¤)_

where each Lğ‘™ is a pre-norm layer (2).
The encoderâ€™s output is a sequence of vectors in
R[ğ‘‘] . To use it as a language recognizer, we add
( )[âˆ—]
an output layer that converts _ğ‘¤_ to a probability
T ( )

_ğ‘Ë†_ = ğœ(w Â· [T (ğ‘¤)]ğ‘– + ğ‘)

where w R[ğ‘‘], ğ‘ R, and ğ‘– is a distinguished
âˆˆ âˆˆ
position. The encoder accepts iff Ë†ğ‘ â‰¥ 2[1] [.]

Chiang and Cholak (2022) also consider a requirement that an encoder accepts/rejects strings
with bounded cross-entropy. That is, we say that
an encoder recognizes a language ğ¿ with crossentropy at most ğœ‚ iff for all strings ğ‘¤, if ğ‘¤ _ğ¿_ then
âˆˆ
log Ë†ğ‘ _ğœ‚, and if ğ‘¤_ âˆ‰ _ğ¿_ then log 1 _ğ‘Ë†_ _ğœ‚._
âˆ’ â‰¤ âˆ’ ( âˆ’ ) â‰¤
We are aware of two choices for the distinguished position ğ‘–. Most papers use the last position
(ğ‘– = ğ‘› 1), but some (Chiang and Cholak, 2022;
âˆ’
Chiang et al., 2023), inspired by binary classifiers
based on BERT (Devlin et al., 2019), prepend a
special symbol CLS at position 0 and use ğ‘– = 0.
While this is a minor difference, it should be noted
that the guarantee of exactly one occurrence of CLS
in the input can be useful in some constructions.

**4.3.2** **Transformer decoders**
A transformer decoder is a transformer encoder
with future masking in its attention, typically
T
used to generate rather than recognize strings. The
input is the prefix of previously-generated symbols,
_ğ‘¤<ğ‘¡_ = ğ‘¤0 Â· Â· Â· ğ‘¤ğ‘¡ âˆ’1, and the output is a probability
distribution Ë†ğ‘(ğ‘¤ğ‘¡ | ğ‘¤<ğ‘¡ ) over the next symbol,

_ğ‘Ë†(Â· | ğ‘¤<ğ‘¡_ ) = S(W [T (ğ‘¤<ğ‘¡ )]ğ‘¡ âˆ’1 + b)

where W âˆˆ R[|][Î£][|Ã—][ğ‘‘] and b âˆˆ R[|][Î£][|]. We assume ğ‘¤0 =
BOS and every string ends with EOS, where BOS and


4


-----

EOS are special symbols that do not occur anywhere
else. To sample a string, we first sample ğ‘¤1 from
_ğ‘Ë†(ğ‘¤1 | BOS), then, for each time step ğ‘¡> 1, sample_
_ğ‘¤ğ‘¡_ from Ë†ğ‘(ğ‘¤ğ‘¡ | ğ‘¤<ğ‘¡ ). The process stops when
_ğ‘¤ğ‘¡_ = EOS. Because each sampled output symbol
becomes part of the input at the next time step, this
kind of model is called autoregressive.
While a decoder can be used to recognize strings
similarly to an encoder, it can also be used to generate the entire string; at least two definitions have
been given for this.
First, Hahn (2020) considers a weighted language as a distribution over strings ğ‘ _ğ‘¤_ . For any
( )
length ğ‘¡, the KL divergence (relative entropy) of the
model Ë†ğ‘ _ğ‘¤_ from the true distribution ğ‘ _ğ‘¤_, for
( ) ( )
predicting ğ‘¤ğ‘¡ conditioned on all previous words, is


âˆ‘ï¸
Î”ğ‘¡ [ Ë†ğ‘ âˆ¥ _ğ‘] =_

_ğ‘¤<ğ‘¡_


âˆ‘ï¸

_ğ‘(ğ‘¤<ğ‘¡_ _ğ‘¤ğ‘¡_ ) log _[ğ‘][(][ğ‘¤][ğ‘¡]_ [|][ ğ‘¤][<ğ‘¡] [)]
_ğ‘¤ğ‘¡_ _ğ‘Ë†(ğ‘¤ğ‘¡_ | ğ‘¤<ğ‘¡ ) _[.]_


As Hahnâ€™s results are negative, he does not spell
out a positive criterion, but he seems to implicitly
require that this divergence vanish at infinity:

lim (5)
_ğ‘¡â†’âˆ_ [Î”][ğ‘¡] [[][ Ë†][ğ‘] [âˆ¥] _[ğ‘][]][ =][ 0][.]_

Second, let us say that a transformer decoder
_ğœ€-generates ğ¿_ iff

_ğ¿_ = {ğ‘¤ | âˆ€ğ‘¡ âˆˆ[|ğ‘¤|], Ë†ğ‘(ğ‘¤ğ‘¡ | ğ‘¤<ğ‘¡ ) â‰¥ _ğœ€}._

Then Yao et al. (2021), following Hewitt et al.
(2020), say that a transformer decoder ğ‘‡ generates a language ğ¿ iff there exists an ğœ€> 0 such that
_ğ‘‡ğœ€-generates ğ¿. (This means that a transformer_
decoder may generate more than one language, depending on the ğœ€ chosen.) They also show that any
_ğœ€-generator can be converted into a recognizer._
While not focusing on transformers, Lin et al.

(2021) demonstrate limitations of autoregressive
models for generation; for example, that there is
a language ğ¿ P that cannot be ğœ€-generated in
âˆˆ
polynomial time for any ğœ€> 0 if P â‰  NP.

**4.3.3** **Transformer encoderâ€“decoders**
A transformer encoderâ€“decoder combines a transformer encoder and decoder, adding to each layer
of the decoder an additional attention sublayer,
known as cross attention, which attends to the output of the encoder. In the literature surveyed here,
only the construction of PÃ©rez et al. (2021) and
related constructions (Bhattamishra et al., 2020b;

Wei et al., 2022a) employ an encoderâ€“decoder.


**4.3.4** **Intermediate steps**
When a transformer decoder or encoderâ€“decoder
is run as a language recognizer, it allows for the
possibility of inserting a number of intermediate
time steps between the end of the input string and
the decision. The encoderâ€“decoder models above
do this, as do some decoder-only models (Feng
et al., 2023; Merrill and Sabharwal, 2024). As we
will see (Â§6.1), intermediate steps vastly increase
the modelâ€™s power, which has also been observed
in practice in the form of a â€œscratchpadâ€ (Nye et al.,
2022) or â€œchain of thoughtâ€ (Wei et al., 2022b).

**4.4** **Uniformity and precision**

Although meaningful theoretical claims can be
made about transformers for fixed-length strings
(e.g. Yun et al., 2020), it is crucial when examining transformers as language recognizers to allow
for unbounded string length. Fixing a maximum
length makes all languages finite, collapsing many
language classes into one.
It might be objected that considering unbounded
lengths is too abstract, because in practice one
can always fix a maximum length. But this maximum length, driven by practical needs, is growing
steadily: for example, GPT-4 Turbo uses 128,000
tokens of context. At the same time, some theoretical findings surveyed here seem to have practical
consequences for modest string lengths. For example, we will see that there are reasons to think that
in theory, transformers cannot recognize PARITY;
in practice, they fail to learn PARITY for strings
with lengths in 2, 50 (Bhattamishra et al., 2020a).
[ ]
Some theoretical studies of transformers do allow them to depend on the input length ğ‘›. To borrow a term from circuit complexity (Â§5.2), they
allow certain kinds of non-uniformity. As we have
seen, some position embeddings (Â§4.1) depend on
_ğ‘›. We discuss some other instances below._

**Numeric precision** Transformers operate, in
principle, on real numbers. While hard attention
transformers could be defined using only rational
numbers, even rational numbers can represent an arbitrary amount of information. With RNNs, the use
of real or rational numbers has led to results that
make them appear more powerful in theory than
in practice (Siegelmann and Sontag, 1994, 1995;

Weiss et al., 2018).
Consequently, many studies use limitedprecision numbers. Some studies limit number
representations to have ğ‘‚ 1 bits, as floating-point
( )


5


-----

numbers do in practice (Chiang et al., 2023). But
Merrill and Sabharwal (2023b) argue that in ğ‘‚ 1
( )
precision, attention cannot attend uniformly to a
string of sufficient length ğ‘›, as the attention weights
(ğ›¼) would all round down to zero. So ğ‘‚ log ğ‘› bits
( )
of precision is a common choice (Yao et al., 2021;
Merrill and Sabharwal, 2023a,b). Other choices are
possible as well: Merrill and Sabharwal (2023a)
use the set F = _ğ‘_ 2[ğ‘] _ğ‘_ Z, ğ‘ N .
{ / | âˆˆ âˆˆ }
Restricting intermediate activations to limited
precision introduces many decisions about when
and how rounding should take place, which can
potentially affect expressivity. For example, when
summing ğ‘› numbers, one could round after each
addition or only at the end of the summation. Better
formalizing these decisions and their impact on
expressivity is an area for future research.

**Parameters** A few constructions allow the parameters themselves to depend on ğ‘›, which we
consider to be a stronger dependence, because if
these transformers were to be learned from data,
different transformers would have to be learned for
different maximum lengths. Finally, a few papers
construct transformers in which ğ‘‘, and therefore
the number of parameters, depends on ğ‘›, which we
consider to be stronger still.

**4.5** **Summary**

In summary, transformers can vary in at least the
following ways, any of which could a priori impact
theoretical claims:

  - Architecture: encoder-only, decoder-only, or
encoderâ€“decoder

  - For encoders: definition of recognition

  - For decoders and encoderâ€“decoders: definition of generation and how many intermediate
steps

  - Position embedding (PE)

  - Attention pattern: leftmost-hard, rightmosthard, average-hard, or softmax

  - Attention masking: none, future, or past

  - Layernorm: inclusion or omission, value of
_ğœ€_
N

  - Residual connections: pre-norm or post-norm

  - Precision: infinite, ğ‘‚ log ğ‘›, ğ‘‚ 1
( ) ( )



  - Uniformity: whether parameter values or number of parameters depend on ğ‘›.

### 5 Languages and Language Classes

Next, we present various formal models that transformers are compared to in the literature surveyed.

**5.1** **Automata and classes L, NL, P**

We assume familiarity with finite automata and Turing machines; for definitions, please see the textbook by Sipser (2013). Counter machines are automata with integer-valued registers (Fischer et al.,

1968); they have been studied extensively in connection with LSTM RNNs (Weiss et al., 2018; Suzgun et al., 2019; Merrill, 2019, 2020).
The language classes L (languages decidable
in ğ‘‚ log ğ‘› space) and P (languages decidable
( )
in polynomial time) are defined using deterministic Turing machines (with a read-only input
tape and a read/write work tape). The class NL
(languages decidable in nondeterministic ğ‘‚ log ğ‘›
( )
space) uses nondeterministic Turing machines. The
class DLOGTIME (languages decidable in ğ‘‚ log ğ‘›
( )
time) uses random-access Turing machines (Barrington et al., 1990). It is known that

L âŠ† NL âŠ† P

but none of these inclusions are known to be strict.

**5.2** **Circuits and classes AC[0], ACC[0], TC[0], NC[1]**

Circuits are a model of parallel computation particularly relevant to transformers. For more details,
please see the textbook by Arora and Barak (2009).
Circuits operate on binary values. If we choose
a fixed-length encoding of the symbols of Î£ as
strings of ğ‘ = âŒˆlog2 |Î£|âŒ‰ bits, then a circuit can
simulate input alphabet Î£ by encoding the value of
the ğ‘–-th input symbol into positions ğ‘–ğ‘ to ğ‘–ğ‘ _ğ‘_ 1 .
+( âˆ’ )
For the rest of this section, we assume Î£ = 0, 1 .
{ }

**Circuits** A circuit ğ¶ with input length ğ‘› is
a directed acyclic graph with ğ‘› _input vertices_
_ğ‘ 1, . . ., ğ‘ ğ‘›_ and zero or more gate vertices, each labeled with a type NOT, AND, or OR. Input vertices
have fan-in (in-degree) zero, NOT gates have fanin one, and the fan-in of AND and OR gates can
be either two or unbounded. One (input or gate)
vertex ğ‘¡ is designated the output of the circuit.
Given an input string ğ‘¤ 0, 1, each input verâˆˆ{ }[ğ‘›]
tex ğ‘ ğ‘– is assigned the value ğ‘¤ğ‘–, and each gate vertex
is assigned the value computed by applying the logical function corresponding to its type to the values


6


-----

assigned to its in-neighbors. The circuit computes
the boolean function ğ¶ : 0, 1 0, 1, map{ }[ğ‘›] â†’{ }
ping each input string to the value assigned to ğ‘¡.
The depth of ğ¶, denoted ğ· _ğ¶_, is the length of the
( )
longest directed path from any ğ‘ ğ‘– to ğ‘¡. The size of
_ğ¶, denoted_ _ğ¶_, is the number of vertices in ğ¶.
| |

**Circuit families** A circuit family is a sequence
C = {ğ¶ğ‘›}ğ‘›âˆˆN such that for each ğ‘›, ğ¶ğ‘› is a circuit
with input length ğ‘›. We treat as a function on
C
0, 1 as follows: for every ğ‘¤ 0, 1, _ğ‘¤_ =
{ }[âˆ—] âˆˆ{ }[âˆ—] C( )
_ğ¶|ğ‘¤_ | (ğ‘¤). Then C defines the language ğ¿ (C) =
_ğ‘¤_ 0, 1 _ğ‘¤_ = 1, and we say that
{ âˆˆ{ }[âˆ—] | C( ) } C
recognizes ğ¿ . The depth and size of are the
(C) C
functions ğ‘› â†¦â†’ _ğ·_ (ğ¶ğ‘›) and ğ‘› â†¦â†’|ğ¶ğ‘›|.

**Uniformity** As defined, a circuit family contains
a different circuit for each length ğ‘›, with no constraint on the relationship between the circuits. For
example, let ğ¿ be any unary language: ğ¿ 1 .
âŠ†{ }[âˆ—]
For ğ‘› âˆˆ N, if 1[ğ‘›] âˆ‰ _ğ¿, define ğ¶ğ‘›_ to be a circuit
for the constant 0 function (an OR gate with fan-in
0), and if 1[ğ‘›] âˆˆ _ğ¿, define ğ¶ğ‘›_ to be a circuit for the
AND of all the inputs. Thus, every unary language,
even an undecidable one, is recognized by a circuit
family of size ğ‘‚ _ğ‘›_ and depth ğ‘‚ 1 .
( ) ( )
A uniformity restriction on a circuit family
{ğ¶ğ‘›}ğ‘›âˆˆN requires that the task of constructing a
description of the circuit ğ¶ğ‘› given input ğ‘› be computable within some specified resource bound as
a function of ğ‘›, potentially making it comparable
with classes defined by bounds on Turing machine
time or space. Two such uniformity bounds are
used in the work here: L and DLOGTIME. Because
these bounds are very restrictive, a special representation of the circuit ğ¶ğ‘› is used, namely, the ability
to answer queries of the type of a gate and whether
the output of one gate is an input to another gate.
We assume that the vertices of the circuit ğ¶ğ‘› are
numbered from 0 to |ğ¶ğ‘›| âˆ’ 1. The direct connec_tion language of a family of circuits_ is the set of
C
all tuples âŸ¨ _ğ‘“, ğ‘–, ğ‘—, 1[ğ‘›]âŸ©_ such that in ğ¶ğ‘›, vertex ğ‘– has
type ğ‘“ and there is an edge from vertex ğ‘– to vertex
_ğ‘—_ (Barrington et al., 1990). Given a computable
function bounding the size of and access to a
C
membership oracle for the direct connection language, for any ğ‘› it is straightforward to write out
the list of vertices, edges, and types in ğ¶ğ‘›.
Then a circuit family is L-uniform (resp.,
C
DLOGTIME-uniform) if there is a Turing machine
that runs in logarithmic space (resp., deterministic logarithmic time) to decide membership in the


direct connection language of .
C

**Circuit complexity classes** Circuit complexity
classes classify circuit families and the languages
they recognize based on uniformity, depth, size,
fan-in bound, and the allowed gates. Since transformers have constant depth, circuit classes with
constant depth are of particular interest; the classes
that are used in the work we survey are:

  - AC[0] contains those languages that can be
recognized by families of circuits with unbounded fan-in, constant depth, and polynomial size.

  - ACC[0] is like AC[0], but also has gates that output 1 iff the inputs sum to 0 modulo some
constant.

  - TC[0] is like AC[0], but also allows MAJORITY
gates, which have unbounded fan-in and output 1 iff at least half of their inputs are 1.

  - NC[1] is like AC[0], but with fan-in at most 2 and
depth in ğ‘‚ log ğ‘› .
( )

The known relationships between these classes are:

AC[0] âŠŠ ACC[0] TC[0] NC[1]
âŠ† âŠ†

in the DLOGTIME-uniform, L-uniform, and nonuniform settings; moreover, L-uniform NC[1] L.
âŠ†

**5.3** **Logic**

A formal language can also be defined as a set
of finite strings that satisfy a closed formula of a
logic. For more details, refer to Thomas (1997) or
Straubing (1994).
In the first-order logic of strings, or FO, the formulas are the smallest set containing:

  - Variables ğ‘¥, ğ‘¦, and so on.

  - Atomic formulas ğ‘„ _ğ‘_ (ğ‘¥), ğ‘¥ = ğ‘¦, ğ‘¥< ğ‘¦, where
_ğ‘_ Î£ is a symbol and ğ‘¥, ğ‘¦ are variables.
âˆˆ

  - ğœ™1 âˆ§ _ğœ™2, ğœ™1 âˆ¨_ _ğœ™2, ğœ™1 â†’_ _ğœ™2, Â¬ğœ™1, where ğœ™1_
and ğœ™2 are formulas.

  - _ğ‘¥.ğœ™,_ _ğ‘¥.ğœ™, where ğ‘¥_ is a variable and ğœ™ is a
âˆ€ âˆƒ
formula.

Under the intended interpretation, variables stand
for positions of a finite string ğ‘¤, and ğ‘„ _ğ‘_ (ğ‘¥) is
true iff ğ‘¤ _ğ‘¥_ = ğ‘. For example, if Î£ = {ğ‘, ğ‘},
âˆ€ğ‘¥.âˆ€ğ‘¦.ğ‘„ _ğ‘_ (ğ‘¥) âˆ§ _ğ‘„ğ‘_ (ğ‘¦) â†’ _ğ‘¥< ğ‘¦_ defines the regular


7


-----

language ğ‘[âˆ—]ğ‘[âˆ—]. The language defined by a closed
formula ğœ™ consists of those strings that satisfy ğœ™.
The languages definable in FO are exactly
the star-free languages (McNaughton and Papert,

1971). Other variants add more quantifiers:

  - FOC adds counting quantifiers _ğ‘¦.ğœ™, which_
âˆƒ[=][ğ‘¥]
hold iff there are exactly ğ‘¥ values of ğ‘¦ that
make ğœ™ true (Barrington et al., 1990).

  - FOM adds majority quantifiers Mğ‘¥.ğœ™, which
hold iff at least half of the values of ğ‘¥ make ğœ™
true (Barrington et al., 1990).

We are also interested in various sets of predicates:

  - Modular predicates MOD[ğ‘Ÿ]ğ‘š[(][ğ‘¥][)][, which hold iff]
_ğ‘¥_ _ğ‘Ÿ_ mod ğ‘š (Barrington et al., 1992).
â‰¡ ( )

  - BIT _ğ‘¥, ğ‘¦_, which holds iff the ğ‘¦-th bit of ğ‘¥ is 1.
( )

  - Mon, the set of all predicates on one position,
possibly depending on ğ‘›.[2]

  - ARB, the set of all predicates on one or more
positions.

A logic extended with predicates is conventionally
written with the predicates in square brackets; for
example, we write FO BIT for first-order logic

[ ]
with the BIT predicate.
In linear temporal logic or LTL (Kamp, 1968),
every formula implicitly depends on a single time
(or position). There are atomic formulas ğ‘„ _ğ‘_ for
every ğ‘ Î£, the connectives,, and, as well as
âˆˆ âˆ§ âˆ¨ Â¬
operators since and until. The formula ğ›¼ **since ğ›½**
is true iff ğ›¼ was true at some past time ğ‘– and ğ›½ was
true from ğ‘– to now (exclusive). LTL is equivalent
to FO (Kamp, 1968).

**5.4** **Relationships**

Figure 1, which depicts the relationships between
the language classes defined above, shows that the
classes defined by circuits/logics cut across the
(perhaps more familiar) Chomsky hierarchy. In
this figure and in this section, all circuit classes
are understood to be DLOGTIME-uniform unless
specified otherwise.

2Although Barrington et al. (2005) define Mon to be the
collection of all monadic predicates without dependence on ğ‘›,
BarcelÃ³ et al. (2024) do allow them to depend on ğ‘›.


free _LP_ FOM[BIT]

SHUFFLE-DYCK-2

BFVP

regular AC[0]

W(S5) MAJORITYDYCK-k _ww_ _a[2][n]_ FO[BIT]

PARITY _ww[R]_

DYCK-(k, D)

Figure 1: Relationship of some languages and language
classes discussed in this paper (right) to the Chomsky
hierarchy (left), assuming that TC[0] âŠŠ NC[1] and L âŠŠ NL.
Circuit classes are DLOGTIME-uniform.

**5.4.1** **Beyond AC[0]**

The classic examples of languages not in AC[0] are
PARITY and MAJORITY. The language PARITY
âŠ†
0, 1 contains all bit strings containing an odd
{ }[âˆ—]
number of 1â€™s, and MAJORITY 0, 1 consists
âŠ†{ }[âˆ—]
of all bit strings in which more than half of the
bits are 1â€™s. Other problems in TC[0] but not AC[0]

include sorting, integer multiplication (Chandra
et al., 1984), and integer division (Hesse, 2001).

**Dyck languages** The language DYCK-ğ‘˜ for ğ‘˜>
0 is the language of strings over ğ‘˜ pairs of parentheses that are correctly balanced and nested. If
we write the ğ‘–-th parenthesis pair as (ğ‘– )ğ‘– for each
_ğ‘–_ _ğ‘˜_, then DYCK-ğ‘˜ is generated by the contextâˆˆ[ ]
free grammar {ğ‘† â†’ (ğ‘–ğ‘†)ğ‘–ğ‘† | ğ‘– âˆˆ[ğ‘˜]} âˆª{ğ‘† â†’
_ğœ€_ . These languages are of interest because any
}
context-free language can be obtained by applying a string homomorphism to the intersection of a
Dyck language with a regular language (Chomsky
and SchÃ¼tzenberger, 1963).
Some papers surveyed here consider variations
on Dyck languages. The language DYCK- _ğ‘˜, ğ·_
( )
for ğ·> 0 is the subset of DYCK-ğ‘˜ consisting of
strings with maximum nesting depth ğ·; it is a starfree regular language (and therefore in AC[0]).
The language SHUFFLE-DYCK-ğ‘˜ is the set of
strings over ğ‘˜ pairs of parentheses in which, for
each parenthesis pair, erasing the other types of
parentheses leaves a correctly balanced and nested
string. For example, is in SHUFFLE-DYCK[(()])
2. If ğ‘˜> 1, SHUFFLE-DYCK-ğ‘˜ is not context free.

**5.4.2** **Beyond TC[0]**

As we will see (Â§6.3.2), some transformer variants
lie within TC[0]. What problems lie beyond?

|recursively enumerable|Col2|NC1 TC0 FOM[BIT] AC0 FO[BIT]|
|---|---|---|
|context sensitive context free regular|LP SHUFFLE-DYCK-2 BFVP W(S5) M DA YJO CR KI -T kY ww a2n PARITY wwR DYCK-(k, D)||


8


-----

**The word problem for permutation groups** A
permutation of _ğ‘˜_ is a bijection ğœ‹ : _ğ‘˜_ _ğ‘˜_,
[ ] [ ] â†’[ ]
and ğ‘†ğ‘˜ is the set of all permutations of [ğ‘˜]. Treating ğ‘†ğ‘˜ as an alphabet and compositions of permutations as strings, we can define the language
W(ğ‘†ğ‘˜) of compositions of permutations of [ğ‘˜]
that equal the identity permutation. For example, in ğ‘†3, the permutation (120) maps 0 â†¦â†’ 1,
1 â†¦â†’ 2, and 2 â†¦â†’ 0, so that W(ğ‘†3) contains
120 120 120 but not 120 120 . These
( ) â—¦( ) â—¦( ) ( ) â—¦( )
languages are easy for finite automata to recognize,
but difficult with only fixed computation depth. Indeed, W(ğ‘†5) is complete for NC[1] under AC[0] reductions (Barrington, 1989), so it is not in TC[0],
assuming that TC[0] âŠŠ NC[1] (as is widely believed).
This makes it an example of a regular language that
transformer encoders probably cannot recognize.
The languages W(ğ‘†ğ‘˜) have some relevance to
natural language: they resemble expressions like
_the child of the enemy of Ann where the interpre-_
tation of the child of is (roughly) a permutation of
possible referents (Paperno, 2022), and problems
that have been used to benchmark transformersâ€™
state-tracking abilities (Kim and Schuster, 2023).

**Other languages that are widely believed to be**
not in TC[0] include:

  - The language of closed Boolean formulas that
are true (BFVP) is context-free but complete
for NC[1] under DLOGTIME reductions (Buss,

1987), so it is outside TC[0] if TC[0] âŠŠ NC[1].

  - Undirected graph connectivity is L-complete
under L-uniform NC[1] reductions (Cook and
McKenzie, 1987; Reingold, 2008), so it is
outside L-uniform NC[1] (and therefore outside
TC[0]) if L-uniform NC[1] âŠŠ L.

  - There is a context-free language ğ¿ _ğ‘ƒ_ that is
NL-complete under L reductions (Sudborough,

1975), so it is outside L (and therefore outside
NC[1] and TC[0]) if L âŠŠ NL.

  - Solving systems of linear equalities and universal context-free grammar recognition are
P-complete under L reductions (Jones and
Laaser, 1976; Greenlaw et al., 1995), so they
are outside TC[0] if L âŠŠ P.

  - Matrix permanent is known to be outside of
TC[0] (Allender, 1999).


**5.4.3** **Circuits and logics**

DLOGTIME-uniform AC[0] and TC[0] are equivalent
to FO BIT and FOM BIT, respectively. There are

[ ] [ ]
many such equivalences between circuit classes
and logics. As a rule of thumb, adding unbounded
fan-in gates to a circuit family correlates with
adding quantifiers to the corresponding logic, and
increasing the degree of non-uniformity of a circuit family correlates with adding numerical predicates to the corresponding logic (Barrington and
Immerman, 1994). For example, making AC[0] and
TC[0] completely non-uniform corresponds to adding
arbitrary numerical predicates ARB to FO and
( )
FOM, respectively (Immerman, 1997; Barrington
et al., 1990).
As we will see below, circuits and logics have
their advantages and disadvantages for capturing
the expressivity of transformers. An advantage of
the circuit approach is that they have a more transparent resemblance to transformers. Transformers
are computations with bounded depth, so itâ€™s not
hard to see that they should be computable by circuit families with bounded depth (AC[0] or TC[0]).
On the other hand, an advantage of the logical approach is that if we seek an exact characterization
of transformers, it can be easier in a logic to add or
remove quantifiers or predicates, to limit quantifier
depth or number of variables, to partition terms into
different sorts, and so on, than to make adjustments
to a circuit family.

### 6 Current Results

While this area of research still has many unresolved questions, the emerging picture has three
levels of expressivity. At the upper end are
decoders or encoderâ€“decoders with intermediate
steps; these are equivalent to Turing machines
(Â§6.1). At the lower end are encoders with leftmosthard or rightmost-hard attention; these can recognize only languages in AC[0] (Â§6.2). In the middle
are encoders with average-hard or softmax attention, which are the least well-understood but appear
to lie between AC[0] and TC[0] (Â§6.3).
In this section, â€œtransformerâ€ refers to a transformer encoder unless otherwise indicated.

**6.1** **Decoders with intermediate steps**

PÃ©rez et al. (2021) consider transformer encoderâ€“
decoders with several modifications:

  - The PE includes components ğ‘–, 1 _ğ‘–, and 1_ _ğ‘–[2]._
/ /


9


-----

Lower bound Source PE Attention Notes

âˆ‹ MAJORITY PÃ©rez et al. 2019 none average-hard
âˆ‹ SHUFFLE-DYCK-ğ‘˜ Bhattamishra et al. 2020a none softmax, future mask
âŠ‡ SSCMs Bhattamishra et al. 2020a none softmax, future mask
âˆ‹ DYCK-k Yao et al. 2021 _ğ‘–/ğ‘›, ğ‘–/ğ‘›[3], ğ‘›_ softmax & leftmost-hard
âŠ‡ P PÃ©rez et al. 2021 _ğ‘–, 1/ğ‘–, 1/ğ‘–[2]_ average-hard poly(ğ‘›) steps
âˆ‹ PARITY Chiang and Cholak 2022 _ğ‘–/ğ‘›, (âˆ’1)[ğ‘–]_ softmax
âŠ‡ FOC[MOD; +] Chiang et al. 2023 sinusoidal softmax
âŠ‡ FO[Mon] BarcelÃ³ et al. 2024 arbitrary leftmost-hard
âŠ‡ LTL+C[Mon] BarcelÃ³ et al. 2024 arbitrary average-hard

Upper bound Source Precision Attention Notes

âˆŒ PARITY, DYCK-1 Hahn 2020 R leftmost-hard
âˆŒ PARITY, DYCK-2 Hahn 2020 R softmax, future mask _ğœ€_ N > 0, vanishing KL
âŠ† AC[0] Hao et al. 2022 Q leftmost-hard
âŠ† TC[0] Merrill et al. 2022 F average-hard
âŠ† FOC[MOD; +] Chiang et al. 2023 _ğ‘‚_ (1) softmax
âŠ† L-uniform TC[0] Merrill & Sabharwal 2023a _ğ‘‚_ (log ğ‘›) softmax
âŠ† FOM[BIT] Merrill & Sabharwal 2023b _ğ‘‚_ (log ğ‘›) softmax
âŠ† L-uniform TC[0] Strobl 2023 F average-hard

Equivalent Source PE Attention Notes

= RE PÃ©rez et al. 2021 _ğ‘–, 1/ğ‘–, 1/ğ‘–[2]_ average-hard unbounded steps
= FO Angluin et al. 2023 none rightmost-hard, strict future mask
= FO[MOD] Angluin et al. 2023 sinusoidal rightmost-hard, strict future mask
= FO[Mon] Angluin et al. 2023 arbitrary rightmost-hard, strict future mask
= P Merrill & Sabharwal 2024 none average-hard, future mask poly(ğ‘›) steps

Table 1: Surveyed claims and their assumptions. Please see the main text for full details of assumptions.



  - In self attention, Eq. (3) takes the negative
absolute value of the dot-product, and Eq. (4)
uses average-hard attention.

  - The FFNs use sigmoids instead of ReLUs.

As described above (Â§4.3.3), the decoder is allowed
to run for arbitrarily many time steps until an acceptance criterion is met. Under these assumptions,
transformer encoderâ€“decoders can recognize any
recursively enumerable language.[3] This result uses
arbitrary precision, but as a corollary, they show
that a ğ‘‡ _ğ‘›_ -time-bounded Turing machine can be
( )
simulated in a transformer using ğ‘‚ log _ğ‘‡_ _ğ‘›_ pre( ( ))
cision and ğ‘‚ _ğ‘‡_ _ğ‘›_ intermediate steps.
( ( ))

Bhattamishra et al. (2020b) provide a simpler
proof of PÃ©rez et al.â€™s result by reducing to an RNN
and appealing to the construction of Siegelmann
and Sontag (1995). They do this for two sets of
assumptions. First,

  - The PE includes only ğ‘–.

  - The self attention sublayers are as above.

3PÃ©rez et al. (2021) define both Turing machines and
encoderâ€“decoders to halt only when accepting. The construction could easily be modified to capture decidable languages.



  - The FFNs use saturated linear activation functions: ğœ _ğ‘¥_ = max 0, min 1, ğ‘¥ .
( ) ( ( ))

Second, they show the same with no PE and standard dot-product attention with future masking.

Wei et al. (2022a) define a notion of statistically_meaningful (SM) approximation and show that_
transformer encoderâ€“decoders SM-approximate
Turing machines. Both the decoder and Turing
machine are limited to ğ‘ time steps; additionally,

  - The PE can be an arbitrary computable function on _ğ‘_ .
[ ]

  - Attention is average-hard.

  - The FFNs have three ReLU layers.

Feng et al. (2023) observe that the problems of
evaluating arithmetic expressions or solving linear
equations over Z _ğ‘_ are NC[1]-hard under DLOGTIME
reductions, so (if TC[0] âŠŠ NC[1]) they cannot be
solved by ğ‘‚ log ğ‘› -precision transformer decoders
( )
without intermediate steps.[4] Similarly, the universal recognition problem for CFGs is P-complete, so

4This uses the result of Merrill and Sabharwal (2023b),
which would have to be adapted to transformer decoders, but
this should be straightforward.


10


-----

(if L âŠŠ P) it cannot be solved by ğ‘‚ log ğ‘› -precision
( )
transformer decoders without intermediate steps.
However, these problems can be solved by a
transformer decoder using (a polynomial number
of) intermediate steps. The decoder has GELU
activations (Hendrycks and Gimpel, 2016) and
PE including ğ‘– and (for linear equation solving)
_ğ‘š[2]_ sin [2][ğ‘–ğœ‹]

_ğ‘š_ [and][ ğ‘š][2][ cos][ 2]ğ‘š[ğ‘–ğœ‹] [where][ ğ‘š] [is the number]

of variables. More generally, they define a class of
dynamic-programming algorithms that these transformers can solve using intermediate steps. All
these decoders have parameters that depend on ğ‘›.

Merrill and Sabharwal (2024) show that a transformer decoder with ğ‘‚ log _ğ‘›_ _ğ‘‡_ _ğ‘›_ precision
( ( + ( )))
and ğ‘‚ _ğ‘‡_ _ğ‘›_ intermediate steps can simulate a Tur( ( ))
ing machine for ğ‘‡ _ğ‘›_ steps, and in particular, de( )
coders with a polynomial number of intermediate
steps recognize exactly the languages in P. The
proof is similar to that of PÃ©rez et al. (2021), but
uses a standard definition of transformers without
PEs, relying only on the mild assumption that the
input string begins with BOS.

**6.2** **Leftmost-hard/rightmost-hard attention**

Hahn (2020) shows that leftmost-hard attention
transformers cannot recognize PARITY or DYCK-1,
using a variant of Furst et al.â€™s random restriction
method for proving that PARITY is outside of AC[0].

Hao et al. (2022) show more generally that
any language recognized by a transformer with
leftmost-hard attention is in AC[0]. The proof gives
a normal form for transformers with leftmost-hard
attention and uses it to construct an AC[0] circuit
family. It uses the fact that only ğ‘‚ log ğ‘› bits of
( )
information are needed per position.

BarcelÃ³ et al. (2024) give a lower bound on
leftmost-hard-attention transformers with arbitrary
PEs depending on a single position ğ‘– and length
_ğ‘›, including ğ‘–,_ _ğ‘–+11_ [,][ (âˆ’][1][)][ğ‘–][,][ cos][ ğœ‹] [(][1]10[âˆ’][2][âˆ’][ğ‘–] [)], and

sin _[ğœ‹]_ [(][1]10[âˆ’][2][âˆ’][ğ‘–] [)] . They show that these transformers

can recognize any language definable in FO Mon .

[ ]
Their proof converts a FO Mon formula to LTL

[ ]
(Â§5.3), which is simulated in a transformer.

Angluin et al. (2023) exactly characterize
rightmost-hard-attention transformers with strict
future masking. Without PEs, these transformers
recognize exactly the class of star-free languages,
that is, languages definable in FO. With periodic
PEs, they are exactly equivalent to FO MOD, and

[ ]
with arbitrary PEs, they are exactly equivalent to
FO Mon . Strict masking is important, as non
[ ]


strict masking is less expressive. They give two
proofs of the star-free to transformer direction, one
which goes through LTL (Â§5.3) and one which uses
Krohn-Rhodes theory. These proofs use a Booleanvalued version of RASP (Weiss et al., 2021) as an
intermediate representation.

**6.3** **Average-hard and softmax attention**

Theoretical results on average-hard and softmax attention transformers have not yet clearly separated
the two, so we treat them together. Both kinds of
attention enable counting, which can be used to
solve problems like MAJORITY that are outside
AC[0]. But these transformers are no more powerful
than DLOGTIME-uniform TC[0], implying that they
likely cannot solve problems complete for NC[1], L,
and other classes believed to be above TC[0] (Â§5.4).

**6.3.1** **Lower bounds: particular languages**
The languages MAJORITY, DYCK-ğ‘˜, and PARITY
are all not in AC[0], so are interesting test cases.

PÃ©rez et al. (2019) prove that a transformer
encoderâ€“decoder with a trivial decoder and without
any PE recognizes MAJORITY; Merrill et al. (2022)
prove the same for transformer encoders.

Bhattamishra et al. (2020a) prove that
SHUFFLE-DYCK-ğ‘˜ (which equals DYCK-1 when
_ğ‘˜_ = 1) is recognizable by a soft-attention transformer with future masking, no PE, no layernorm,
and no residual connections. Yao et al. (2021)
show that a transformer decoder can generate
DYCK-ğ‘˜ using ğ‘‚ log ğ‘› precision, softmax and
( )
leftmost-hard attention, future masking, and a
PE including ğ‘– _ğ‘›, ğ‘–_ _ğ‘›[3], and ğ‘›._ They also give
/ /
constructions for DYCK- _ğ‘˜, ğ·_ .
( )

Chiang and Cholak (2022) show that transformers whose PE includes ğ‘– _ğ‘›_ and 1 = cos _ğ‘–ğœ‹_ can
/ (âˆ’ )[ğ‘–]
recognize PARITY.
On the other hand, Hahn (2020) shows that softmax attention transformers cannot generate PAR
ITY or DYCK-2 under the following two conditions:

1. all position-wise functions are Lipschitzcontinuous, and

2. generation is defined using the KL divergence
criterion in Eq. (5).

The apparent contradiction is resolved by considering the different assumptions underlying each
result. Chiang and Cholak (2022) address this by
giving two constructions corresponding to Hahnâ€™s
two conditions. The first has Lipschitz-continuous


11


-----

position-wise functions, but has high cross-entropy
(Â§4.3.1); as a generator, it would not meet criterion (5). The second construction uses layernorm
with ğœ€ N = 0, which is not Lipschitz-continuous,
but it has arbitrarily low cross-entropy.
A number of authors have tested empirically
whether transformers can learn the above languages. Ebrahimi et al. (2020) find that they are
competitive with LSTMs at learning DYCK-2 and
DYCK-4, and that prepending a BOS symbol helps.

Bhattamishra et al. (2020a) train transformers
with future masking and no PE on DYCK-1 and
SHUFFLE-DYCK-ğ‘˜, finding near-perfect learning
and length generalization. For the languages
DYCK- 1, ğ· with learned or sinusoidal PEs, they
( )
find that the models do not generalize well for
_ğ·> 1. Yao et al. (2021) then investigate DYCK-_
_ğ‘˜, ğ·_ for several values of ğ‘˜ and ğ· and several
( )
PEs. They report strong generalization only when
using ğ‘– _ğ‘›_ for the PE, and posit that this is the key.
/
It is hard, however, to directly compare the two
results: Bhattamishra et al. (2020a) require correct prediction of the possible next symbols at each
string prefix, while Yao et al. (2021) average over
predictions of right brackets.

DelÃ©tang et al. (2023) study experimentally how
well transformers (and other networks) learn tasks
at various levels of the Chomsky hierarchy, including generalization to longer strings. They find that
transformers learn MAJORITY, but not PARITY.

**6.3.2** **Upper bounds: TC[0]**

Merrill et al. (2022) prove an upper bound analogous to that of Hao et al. (2022), but for averagehard-attention transformers. They show that an
average-hard-attention transformer with activations
in F can be simulated in TC[0]. Strobl (2023) tightens
this bound to L-uniform TC[0].
Furthermore, Merrill and Sabharwal (2023a)
show that softmax attention, ğ‘‚ log ğ‘› -precision
( )
transformers are in L-uniform TC[0], and then tighten
this bound to DLOGTIME-uniform TC[0] (Merrill
and Sabharwal, 2023b). The proof constructs subroutines to answer queries about the types of nodes
and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these
queries can be translated to queries for a TC[0] circuit
family with ğ‘‚ log ğ‘› time overhead.
( )
An upper bound of DLOGTIME-uniform TC[0] immediately implies an upper bound of FOM BIT

[ ]
(Merrill and Sabharwal, 2023b). Chiang et al.

(2023) prove a tighter upper bound using a logic


called FOC MOD;, but on transformers with

[ +]
_ğ‘‚_ 1 precision. This result is discussed below.
( )

**6.3.3** **Other lower bounds**
In addition to explicit constructions for particular
languages mentioned above, various lower bounds
have been proven, which are quite diverse.

**Counter machines** Bhattamishra et al. (2020a),
following Merrill et al. (2020), define a subclass
of counter machines called simplified and stateless
_ğ‘˜-counter machines (SSCMs). These can update_
each counter based on the current input symbol,
but have no state and cannot read the counters until
the end of the string. They show that any SSCM
can be converted to an equivalent transformer with
future masking and no residual connections.

**Finite automata** Liu et al. (2023) study the ability of transformers with future masked attention
to simulate deterministic finite automata (DFAs),
in the sense of computing not only the same acceptance decision but also the same state sequence.
Although a transformer with depth ğ‘ can simulate
a DFA for ğ‘ timesteps, Liu et al. show how to construct lower-depth shortcuts for subclasses roughly
corresponding to classes of regular languages in
Fig. 1. Though the parameters of these constructions depend on ğ‘, in the context of this survey,
a noteworthy finding is that any regular language
in ACC[0] can be recognized up to length ğ‘ by a
transformer whose FFNs use sine activations and
whose number of parameters is independent of ğ‘.

**First-order logic** Chiang et al. (2023) obtain
both an upper and a lower bound by defining a
logic FOC MOD;, which is first-order logic with

[ +]
counting quantifiers, using two sorts for positions
and counts (Immerman, 1999, p. 185â€“187), where
positions have the MOD predicate (but not < or =),
and counts have <,, and =, capturing the fact that
+
transformers can add and compare activations, but
not positions. They show that this logic is intermediate in expressivity between ğ‘‚ 1 -precision and
( )
infinite-precision transformers. The lower-bound
proof uses a normal form that eliminates quantifiers
over counts and makes quantifiers over positions
have depth 1; a perhaps surprising consequence is
that ğ‘‚ 1 -precision transformers are no more pow( )
erful than 2-layer uniform-attention transformers.

**Temporal logic** BarcelÃ³ et al. (2024) show
that average-hard-attention transformers with arbitrary PEs depending on a single position ğ‘– and


12


-----

length ğ‘›, including ğ‘–, _ğ‘–+11_ [,][ (âˆ’][1][)][ğ‘–][,][ cos][ ğœ‹] [(][1]10[âˆ’][2][âˆ’][ğ‘–] [)], and

sin _[ğœ‹]_ [(][1]10[âˆ’][2][âˆ’][ğ‘–] [)], can recognize any language definable

in LTL with counting operators, Presburger arithmetic on counts, and predicates in Mon.

**Programming languages** Weiss et al. (2021) introduce the RASP (Restricted Access Sequence
Processing) language as an abstraction of transformers, discussing how its components relate to
the transformer architecture. However, they do not
prove any relationship.

Lindner et al. (2023) present Tracr, a compiler
from RASP programs to transformers. To do so,
they impose some restrictions: a maximum input
length, given at compile time; a mandatory BOS
token; and the removal of selector composition, a
RASP operation with no clear parallel in transformers. They rewrite several programs from Weiss
et al. (2021) without this operation. In the other
direction, Friedman et al. (2023) define a restricted
class of transformers that can be learned and decompiled into RASP. Finally, Angluin et al. (2023)
use a version of RASP restricted to Boolean values,
and Zhou et al. (2024) use a restricted version of
RASP to explore length generalization.

### 7 Conclusions

Out of the large body of research surveyed above,
we highlight several conclusions:

1. Transformer decoders can use intermediate
steps to simulate Turing machines; with unbounded steps, they are Turing-complete.

2. Regarding the expressivity of transformer encoders, circuit complexity and logic are especially promising frameworks.

3. Leftmost-hard-attention transformer encoders
are in AC[0] and cannot solve some intuitively
easy problems, like PARITY and MAJORITY.

4. Softmax and average-hard attention give transformer encoders the ability to count. Still, they
lie within TC[0] and likely cannot solve problems like evaluating closed Boolean formulas.

Some open questions that we think should be priorities for future research are:

5. Some variants (PEs, average-hard vs. softmax
attention, pre-norm vs. post-norm, the presence of BOS/EOS/CLS) appear to be instrumental in proofs reviewed here; can their effect on
expressivity be clarified?


6. Can the expressivity of softmax-attention
transformers be characterized more tightly or
even exactly in terms of some logic?

7. Given the current practical importance of
decoder-only transformers and chain-ofthought, what further insights can circuits or
logic provide into transformer decoders?

We hope this paper can serve as a valuable resource
for researchers pursuing these and other questions.

### Acknowledgements

We would like to thank Frank Drewes, Jon Rawski,
Ashish Sabharwal, and the anonymous reviewers
for their valuable comments on earlier versions of
this paper.

### References

[Joshua Ackerman and George Cybenko. 2020. A](http://arxiv.org/abs/2006.01338)
[survey of neural networks and formal languages.](http://arxiv.org/abs/2006.01338)
arXiv:2006.01338.

[Zeyuan Allen-Zhu and Yuanzhi Li. 2023. Physics](https://arxiv.org/abs/2305.13673)
[of language models: Part 1, context-free gram-](https://arxiv.org/abs/2305.13673)
[mar. arXiv:2305.13673.](https://arxiv.org/abs/2305.13673)

[Eric Allender. 1999. The permanent requires large](http://cjtcs.cs.uchicago.edu/articles/1999/7/contents.html)
[uniform threshold circuits. Chicago Journal of](http://cjtcs.cs.uchicago.edu/articles/1999/7/contents.html)
_Theoretical Computer Science, 1999(7)._

Dana Angluin, David Chiang, and Andy Yang.
[2023. Masked hard-attention transformers and](https://arxiv.org/abs/2310.13897)
[Boolean RASP recognize exactly the star-free](https://arxiv.org/abs/2310.13897)
[languages. arXiv:2310.13897.](https://arxiv.org/abs/2310.13897)

[Sanjeev Arora and Boaz Barak. 2009. Computa-](http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521424264)
_[tional Complexity: A Modern Approach. Cam-](http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521424264)_
bridge University Press.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E.
[Hinton. 2016. Layer normalization. In NIPS](https://arxiv.org/abs/1607.06450)
_2016 Deep Learning Symposium._

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
[Bengio. 2015. Neural machine translation by](http://arxiv.org/abs/1409.0473)
[jointly learning to align and translate. In Pro-](http://arxiv.org/abs/1409.0473)
_ceedings of the Third International Conference_
_on Learning Representations (ICLR)._

Pablo BarcelÃ³, Alexander Kozachinskiy, Anthony Widjaja Lin, and Vladimir Podolskii. 2024.
[Logical languages accepted by transformer en-](https://openreview.net/forum?id=gbrHZq07mq)
[coders with hard attention. In Proceedings of the](https://openreview.net/forum?id=gbrHZq07mq)


13


-----

_Twelfth International Conference on Learning_
_Representations (ICLR)._

David A. Barrington. 1989. [Bounded-width](https://doi.org/10.1016/0022-0000(89)90037-8)
[polynomial-size branching programs recognize](https://doi.org/10.1016/0022-0000(89)90037-8)
[exactly those languages in NC[1]. Journal of Com-](https://doi.org/10.1016/0022-0000(89)90037-8)
_puter and System Sciences, 38(1):150â€“164._

David A. Barrington, Kevin Compton, Howard
[Straubing, and Denis ThÃ©rien. 1992. Regular](https://doi.org/https://doi.org/10.1016/0022-0000(92)90014-A)
[languages in NC[1]. Journal of Computer and](https://doi.org/https://doi.org/10.1016/0022-0000(92)90014-A)
_System Sciences, 44(3):478â€“499._

David A. Mix Barrington, Neil Immerman,
Clemens Lautemann, Nicole Schweikardt, and
[Denis ThÃ©rien. 2005. First-order expressibility](https://doi.org/10.1016/j.jcss.2004.07.004)
[of languages with neutral letters or: The Crane](https://doi.org/10.1016/j.jcss.2004.07.004)
[Beach conjecture. Journal of Computer and Sys-](https://doi.org/10.1016/j.jcss.2004.07.004)
_tem Sciences, 70(2):101â€“127._

David A. Mix Barrington, Neil Immerman, and
[Howard Straubing. 1990. On uniformity within](https://doi.org/https://doi.org/10.1016/0022-0000(90)90022-D)
_[NC[1]. Journal of Computer and System Sciences,](https://doi.org/https://doi.org/10.1016/0022-0000(90)90022-D)_
41(3):274â€“306.

David Mix Barrington and Neil Immerman. 1994.

[Time, hardware, and uniformity. In Proceedings](https://doi.org/10.1109/SCT.1994.315806)
_of the IEEE 9th Annual Conference on Structure_
_in Complexity Theory, pages 176â€“185._

[Valeriu Beiu and John G. Taylor. 1996. On the cir-](https://doi.org/10.1016/0893-6080(96)00130-X)
[cuit complexity of sigmoid feedforward neural](https://doi.org/10.1016/0893-6080(96)00130-X)
[networks. Neural Networks, 9(7):1155â€“1171.](https://doi.org/10.1016/0893-6080(96)00130-X)

Satwik Bhattamishra, Kabir Ahuja, and Navin
[Goyal. 2020a. On the ability and limitations](https://doi.org/10.18653/v1/2020.emnlp-main.576)
[of Transformers to recognize formal languages.](https://doi.org/10.18653/v1/2020.emnlp-main.576)
In Proceedings of the 2020 Conference on Em_pirical Methods in Natural Language Processing_
_(EMNLP), pages 7096â€“7116._

Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
[2020b. On the computational power of Trans-](https://doi.org/10.18653/v1/2020.conll-1.37)
[formers and its implications in sequence mod-](https://doi.org/10.18653/v1/2020.conll-1.37)
[eling. In Proceedings of the 24th Conference](https://doi.org/10.18653/v1/2020.conll-1.37)
_on Computational Natural Language Learning_
_(CoNLL), pages 455â€“475._

Satwik Bhattamishra, Arkil Patel, Varun Kanade,
and Phil Blunsom. 2023. [Simplicity bias in](https://doi.org/10.18653/v1/2023.acl-long.317)
[Transformers and their ability to learn sparse](https://doi.org/10.18653/v1/2023.acl-long.317)
[Boolean functions. In Proceedings of the 61st](https://doi.org/10.18653/v1/2023.acl-long.317)
_Annual Meeting of the Association for Computa-_
_tional Linguistics (ACL), pages 5767â€“5791._


Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.
[Language models are few-shot learners. In Ad-](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
_vances in Neural Information Processing Sys-_
_tems 33 (NeurIPS), pages 1877â€“1901._

[Samuel R. Buss. 1987. The Boolean formula value](https://doi.org/10.1145/28395.28409)
[problem is in ALOGTIME. In Proceedings of](https://doi.org/10.1145/28395.28409)
_the Nineteenth Annual ACM Symposium on The-_
_ory of Computing (STOC), pages 123â€“131._

Ashok K. Chandra, Larry Stockmeyer, and Uzi
Vishkin. 1984. [Constant depth reducibility.](https://doi.org/10.1137/0213028)
_SIAM J. Computing, 13(2):423â€“439._

David Chiang and Peter Cholak. 2022. [Over-](https://doi.org/10.18653/v1/2022.acl-long.527)
[coming a theoretical limitation of self-attention.](https://doi.org/10.18653/v1/2022.acl-long.527)
In Proceedings of the 60th Annual Meeting of
_the Association for Computational Linguistics_
_(ACL), pages 7654â€“7664._

David Chiang, Peter Cholak, and Anand Pillay.
2023. [Tighter bounds on the expressivity of](https://proceedings.mlr.press/v202/chiang23a.html)
[transformer encoders. In Proceedings of the 40th](https://proceedings.mlr.press/v202/chiang23a.html)
_International Conference on Machine Learning_
_(ICML), volume 202 of Proceedings of Machine_
_Learning Research, pages 5544â€“5562._

[N. Chomsky and M. P. SchÃ¼tzenberger. 1963. The](https://doi.org/10.1016/S0049-237X(08)72023-8)
[algebraic theory of context-free languages. In](https://doi.org/10.1016/S0049-237X(08)72023-8)
P. Braffort and D. Hirschberg, editors, Computer
_Programming and Formal Systems, volume 35_
of Studies in Logic and the Foundations of Math_ematics, pages 118â€“161. Elsevier._

Stephen A. Cook and Pierre McKenzie. 1987.

[Problems complete for deterministic logarithmic](https://doi.org/10.1016/0196-6774(87)90018-6)
[space. Journal of Algorithms, 8(3):385â€“394.](https://doi.org/10.1016/0196-6774(87)90018-6)

[G. Cybenko. 1989. Approximation by superposi-](https://doi.org/10.1007/BF02551274)
[tions of a sigmoidal function. Mathematics of](https://doi.org/10.1007/BF02551274)
_Control, Signals, and Systems, 2(4):303â€“314._

GrÃ©goire DelÃ©tang, Anian Ruoss, Jordi Grau-Moya,
Tim Genewein, Li Kevin Wenliang, Elliot Catt,


14


-----

Chris Cundy, Marcus Hutter, Shane Legg, Joel
[Veness, and Pedro A. Ortega. 2023. Neural net-](https://openreview.net/forum?id=WbxHAzkeQcn)
[works and the Chomsky hierarchy. In Proceed-](https://openreview.net/forum?id=WbxHAzkeQcn)
_ings of the Eleventh International Conference on_
_Learning Representations (ICLR)._

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
[Kristina Toutanova. 2019. BERT: Pre-training](https://aclanthology.org/N19-1423)
[of deep bidirectional Transformers for language](https://aclanthology.org/N19-1423)
[understanding. In Proceedings of the 2019 Con-](https://aclanthology.org/N19-1423)
_ference of the North American Chapter of the As-_
_sociation for Computational Linguistics: Human_
_Language Technologies (NAACL HLT), pages_
4171â€“4186.

Javid Ebrahimi, Dhruv Gelda, and Wei Zhang.
[2020. How can self-attention networks recog-](https://doi.org/10.18653/v1/2020.findings-emnlp.384)
[nize Dyck-n languages? In Findings of the Asso-](https://doi.org/10.18653/v1/2020.findings-emnlp.384)
_ciation for Computational Linguistics: EMNLP_
_2020, pages 4301â€“4306._

Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian
[Ye, Di He, and Liwei Wang. 2023. Towards](https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)
[revealing the mystery behind Chain of Thought:](https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)
[A theoretical perspective. In Advances in Neural](https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)
_Information Processing Systems 36 (NeurIPS),_
pages 70757â€“70798.

Patrick C. Fischer, Albert R. Meyer, and Arnold L.
[Rosenberg. 1968. Counter machines and counter](https://doi.org/10.1007/BF01694011)
[languages.](https://doi.org/10.1007/BF01694011) _Mathematical Systems Theory,_
2:265â€“283.

Dan Friedman, Alexander Wettig, and Danqi Chen.
[2023. Learning Transformer programs. In Ad-](https://papers.nips.cc/paper_files/paper/2023/hash/995f693b73050f90977ed2828202645c-Abstract-Conference.html)
_vances in Neural Information Processing Sys-_
_tems 36 (NeurIPS), pages 49044â€“49067._

Merrick Furst, James B. Saxe, and Michael Sipser.
[1984. Parity, circuits, and the polynomial-time](https://doi.org/10.1007/BF01744431)
[hierarchy. Mathematical Systems Theory, 17:13â€“](https://doi.org/10.1007/BF01744431)
27.

Raymond Greenlaw, H. James Hoover, and Walter L. Ruzzo. 1995. Limits to Parallel Computa_tion: P-Completeness Theory. Oxford Univer-_
sity Press. Preliminary version of Appendix A
[available as Technical Report TR91-11, Univer-](https://doi.org/10.7939/R39Z90F7X)
[sity of Alberta, Department of Computing Sci-](https://doi.org/10.7939/R39Z90F7X)
[ence.](https://doi.org/10.7939/R39Z90F7X)

[Michael Hahn. 2020. Theoretical limitations of](https://doi.org/10.1162/tacl_a_00306)
[self-attention in neural sequence models. Trans-](https://doi.org/10.1162/tacl_a_00306)
_actions of the Association for Computational_
_Linguistics, 8:156â€“171._


Yiding Hao, Dana Angluin, and Robert Frank.
[2022. Formal language recognition by hard at-](https://doi.org/10.1162/tacl_a_00490)
[tention Transformers: Perspectives from circuit](https://doi.org/10.1162/tacl_a_00490)
[complexity. Transactions of the Association for](https://doi.org/10.1162/tacl_a_00490)
_Computational Linguistics, 10:800â€“810._

[Dan Hendrycks and Kevin Gimpel. 2016. Gaussian](https://arxiv.org/abs/1606.08415)
[error linear units (GELUs). arXiv:1606.08415.](https://arxiv.org/abs/1606.08415)

William Hesse. 2001. [Division is in uniform](https://doi.org/10.1007/3-540-48224-5_9)
[TC[0]. In Automata, Languages and Program-](https://doi.org/10.1007/3-540-48224-5_9)
_ming (ICALP), pages 104â€“114. Springer._

John Hewitt, Michael Hahn, Surya Ganguli, Percy
Liang, and Christopher D. Manning. 2020.
[RNNs can generate bounded hierarchical lan-](https://doi.org/10.18653/v1/2020.emnlp-main.156)
[guages with optimal memory. In Proceedings of](https://doi.org/10.18653/v1/2020.emnlp-main.156)
_the 2020 Conference on Empirical Methods in_
_Natural Language Processing (EMNLP), pages_
1978â€“2010.

Kurt Hornik, Maxwell B. Stinchcombe, and Hal[bert White. 1989. Multilayer feedforward net-](https://doi.org/10.1016/0893-6080(89)90020-8)
[works are universal approximators. Neural Net-](https://doi.org/10.1016/0893-6080(89)90020-8)
_works, 2(5):359â€“366._

Austin Huang, Suraj Subramanian, Jonathan Sum,
Khalid Almubarak, and Stella Biderman. 2022.
[The annotated Transformer. Based on original](http://harvardnlp.github.io/annotated-transformer)
version by Sasha Rush.

[Neil Immerman. 1997. Languages that capture](https://doi.org/10.1137/0216051)
[complexity classes. SIAM Journal on Comput-](https://doi.org/10.1137/0216051)
_ing, 16(4):760â€“778._

Neil Immerman. 1999. Descriptive Complexity.
Springer.

[Neil D. Jones and William T. Laaser. 1976. Com-](https://doi.org/10.1016/0304-3975(76)90068-2)
[plete problems for deterministic polynomial](https://doi.org/10.1016/0304-3975(76)90068-2)
[time. Theoretical Computer Science, 3(1):105â€“](https://doi.org/10.1016/0304-3975(76)90068-2)
117.

[Johan Anthony Willem Kamp. 1968. Tense Logic](https://www.proquest.com/docview/302320357)
_[and the Theory of Linear Order. Ph.D. thesis,](https://www.proquest.com/docview/302320357)_
University of California, Los Angeles.

[Najoung Kim and Sebastian Schuster. 2023. Entity](https://doi.org/10.18653/v1/2023.acl-long.213)
[tracking in language models. In Proceedings](https://doi.org/10.18653/v1/2023.acl-long.213)
_of the 61st Annual Meeting of the Association_
_for Computational Linguistics (Volume 1: Long_
_Papers), pages 3835â€“3855._

Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R.
[Gormley, and Jason Eisner. 2021. Limitations](https://doi.org/10.18653/v1/2021.naacl-main.405)


15


-----

[of autoregressive models and their alternatives.](https://doi.org/10.18653/v1/2021.naacl-main.405)
In Proceedings of the 2021 Conference of the
_North American Chapter of the Association for_
_Computational Linguistics: Human Language_
_Technologies (NAACL HLT), pages 5147â€“5173._

Tianyang Lin, Yuxin Wang, Xiangyang Liu, and
[Xipeng Qiu. 2022. A survey of transformers. AI](https://doi.org/10.1016/j.aiopen.2022.10.001)
_Open, 3:111â€“132._

David Lindner, JÃ¡nos KramÃ¡r, Matthew Rahtz,
Thomas McGrath, and Vladimir Mikulik. 2023.
[Tracr: Compiled transformers as a laboratory](https://papers.nips.cc/paper_files/paper/2023/hash/771155abaae744e08576f1f3b4b7ac0d-Abstract-Conference.html)
[for interpretability. In Advances in Neural Infor-](https://papers.nips.cc/paper_files/paper/2023/hash/771155abaae744e08576f1f3b4b7ac0d-Abstract-Conference.html)
_mation Processing Systems 36 (NeurIPS), pages_
37876â€“37899.

Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay
[Krishnamurthy, and Cyril Zhang. 2023. Trans-](https://openreview.net/forum?id=De4FYqjFueZ)
[formers learn shortcuts to automata. In Proceed-](https://openreview.net/forum?id=De4FYqjFueZ)
_ings of the Eleventh International Conference on_
_Learning Representations (ICLR)._

Robert McNaughton and Seymour A. Papert. 1971.

_[Counter-Free Automata. MIT Press.](https://archive.org/details/CounterFre_00_McNa)_

[William Merrill. 2019. Sequential neural networks](https://doi.org/10.18653/v1/W19-3901)
[as automata. In Proceedings of the Workshop on](https://doi.org/10.18653/v1/W19-3901)
_Deep Learning and Formal Languages: Building_
_Bridges, pages 1â€“13._

William Merrill. 2020. On the linguis[tic capacity of real-time counter automata.](https://arxiv.org/abs/2004.06866)
arXiv:2004.06866.

[William Merrill. 2021. Formal language theory](https://arxiv.org/abs/2102.10094)
[meets modern NLP. arXiv:2102.10094.](https://arxiv.org/abs/2102.10094)

[William Merrill. 2023. Formal languages and the](https://doi.org/10.1007/978-3-031-33264-7_1)
[NLP black box. In Developments in Language](https://doi.org/10.1007/978-3-031-33264-7_1)
_Theory, pages 1â€“8._

William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. 2021.
[Effects of parameter norm growth during trans-](https://doi.org/10.18653/v1/2021.emnlp-main.133)
[former training: Inductive bias from gradient](https://doi.org/10.18653/v1/2021.emnlp-main.133)
[descent. In Proceedings of the 2021 Conference](https://doi.org/10.18653/v1/2021.emnlp-main.133)
_on Empirical Methods in Natural Language Pro-_
_cessing (EMNLP), pages 1766â€“1781._

William Merrill and Ashish Sabharwal. 2023a.

[The parallelism tradeoff: Limitations of log-](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00562/116413/The-Parallelism-Tradeoff-Limitations-of-Log)
[precision transformers. Transactions of the As-](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00562/116413/The-Parallelism-Tradeoff-Limitations-of-Log)
_sociation for Computational Linguistics, 11:531â€“_
545.


[William Merrill and Ashish Sabharwal. 2023b. A](https://papers.nips.cc/paper_files/paper/2023/hash/a48e5877c7bf86a513950ab23b360498-Abstract-Conference.html)
[logic for expressing log-precision transformers.](https://papers.nips.cc/paper_files/paper/2023/hash/a48e5877c7bf86a513950ab23b360498-Abstract-Conference.html)
In Advances in Neural Information Processing
_Systems 36 (NeurIPS), pages 52453â€“52463._

[William Merrill and Ashish Sabharwal. 2024. The](https://openreview.net/forum?id=NjNGlPh8Wh)
[expressive power of transformers with chain of](https://openreview.net/forum?id=NjNGlPh8Wh)
[thought. In Proceedings of the Twelfth Interna-](https://openreview.net/forum?id=NjNGlPh8Wh)
_tional Conference on Learning Representations_
_(ICLR)._

William Merrill, Ashish Sabharwal, and Noah A.
Smith. 2022. [Saturated transformers are](https://doi.org/10.1162/tacl_a_00493)
[constant-depth threshold circuits. Transactions](https://doi.org/10.1162/tacl_a_00493)
_of the Association for Computational Linguistics,_
10:843â€“856.

William Merrill, Gail Weiss, Yoav Goldberg, Roy
Schwartz, Noah A. Smith, and Eran Yahav.
[2020. A formal hierarchy of RNN architectures.](https://doi.org/10.18653/v1/2020.acl-main.43)
In Proceedings of the 58th Annual Meeting of
_the Association for Computational Linguistics_
_(ACL), pages 443â€“459._

Maxwell Nye, Anders Andreassen, Guy GurAri, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz,
Maarten Bosma, David Luan, Charles Sutton,
[and Augustus Odena. 2022. Show your work:](https://openreview.net/forum?id=HBlx2idbkbq)
[Scratchpads for intermediate computation with](https://openreview.net/forum?id=HBlx2idbkbq)
[language models. In Proceedings of the Work-](https://openreview.net/forum?id=HBlx2idbkbq)
_shop on Deep Learning for Code (DL4C)._

OpenAI. 2023. GPT-4 [technical](https://arxiv.org/abs/2303.08774) report.
arXiv:2303.08774.

[Denis Paperno. 2022. On learning interpreted lan-](https://doi.org/10.1162/coli_a_00431)
[guages with recurrent models. Computational](https://doi.org/10.1162/coli_a_00431)
_Linguistics, 48(2):471â€“482._

Ian Parberry. 1994. Circuit Complexity and Neural
_Networks. MIT Press._

Jorge PÃ©rez, Pablo BarcelÃ³, and Javier Marinkovic.
[2021. Attention is Turing-complete. Journal of](http://jmlr.org/papers/v22/20-302.html)
_Machine Learning Research, 22:75:1â€“75:35._

[Mary Phuong and Marcus Hutter. 2022. Formal](http://arxiv.org/abs/2207.09238)
[algorithms for transformers. arXiv:2207.09238.](http://arxiv.org/abs/2207.09238)

Jorge PÃ©rez, Javier MarinkoviÂ´c, and Pablo BarcelÃ³.
[2019. On the Turing completeness of modern](https://openreview.net/forum?id=HyGBdo0qFm)
[neural network architectures. In Proceedings of](https://openreview.net/forum?id=HyGBdo0qFm)
_the Seventh International Conference on Learn-_
_ing Representations (ICLR)._


16


-----

Alec Radford, Karthik Narasimhan, Tim Salimans,
[and Ilya Sutskever. 2018. Improving language](https://openai.com/research/language-unsupervised)
[understanding by generative pre-training.](https://openai.com/research/language-unsupervised)

[Omer Reingold. 2008. Undirected connectivity in](https://doi.org/10.1145/1391289.1391291)
[log-space. Journal of the ACM, 55(4):1â€“24.](https://doi.org/10.1145/1391289.1391291)

Clayton Sanford, Daniel Hsu, and Matus Telgarsky.
[2023. Representational strengths and limitations](https://papers.nips.cc/paper_files/paper/2023/hash/73bf692447f174984f30499ec9b20e04-Abstract-Conference.html)
[of transformers. In Advances in Neural Infor-](https://papers.nips.cc/paper_files/paper/2023/hash/73bf692447f174984f30499ec9b20e04-Abstract-Conference.html)
_mation Processing Systems 36 (NeurIPS), pages_
36677â€“36707.

Hava T. Siegelmann and Eduardo D. Sontag. 1994.

[Analog computation via neural networks. Theo-](https://doi.org/10.1016/0304-3975(94)90178-3)
_retical Computer Science, 131(2):331â€“360._

Hava T. Siegelmann and Eduardo D. Sontag. 1995.

[On the computational power of neural nets. Jour-](https://doi.org/10.1006/jcss.1995.1013)
_nal of Computer and System Sciences, 50(1):132â€“_
150.

JiË‡rÃ­ Å Ã­ma and Pekka Orponen. 2003. [General-](https://doi.org/10.1162/089976603322518731)
[purpose computation with neural networks: A](https://doi.org/10.1162/089976603322518731)
[survey of complexity theoretic results. Neural](https://doi.org/10.1162/089976603322518731)
_Computation, 15(12):2727â€“2778._

Michael Sipser. 2013. Introduction to the Theory
_of Computation, 3rd edition. Cengage Learning._

Kai-Yeung Siu, Vwani Roychowdhury, and
Thomas Kailath. 1995. Discrete Neural Compu_tation. Prentice Hall._

Howard Straubing. 1994. Finite Automata, Formal
_Logic, and Circuit Complexity. Springer._

[Lena Strobl. 2023. Average-hard attention trans-](https://arxiv.org/abs/2308.03212)
[formers are constant-depth uniform threshold](https://arxiv.org/abs/2308.03212)
[circuits. arXiv:2308.03212.](https://arxiv.org/abs/2308.03212)

[I. H. Sudborough. 1975. On tape-bounded com-](https://doi.org/10.1016/S0022-0000(75)80014-6)
[plexity classes and multihead finite automata.](https://doi.org/10.1016/S0022-0000(75)80014-6)
_Journal of Computer and System Sciences,_
10(1):62â€“76.

Mirac Suzgun, Yonatan Belinkov, Stuart Shieber,
and Sebastian Gehrmann. 2019. [LSTM net-](https://doi.org/10.18653/v1/W19-3905)
[works can perform dynamic counting. In Pro-](https://doi.org/10.18653/v1/W19-3905)
_ceedings of the Workshop on Deep Learning and_
_Formal Languages: Building Bridges, pages 44â€“_
54.

[Wolfgang Thomas. 1997. Languages, automata,](https://doi.org/10.1007/978-3-642-59126-6_7)
[and logic.](https://doi.org/10.1007/978-3-642-59126-6_7) In Grzegorz Rozenberg and Arto


Salomaa, editors, Handbook of Formal Lan_guages: Volume 3 Beyond Words, pages 389â€“_
455. Springer.

Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
[Lukasz Kaiser, and Illia Polosukhin. 2017. At-](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
[tention is all you need. In Advances in Neural](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
_Information Processing Systems 30 (NeurIPS)._

Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,
Changliang Li, Derek F. Wong, and Lidia S.
[Chao. 2019. Learning deep Transformer mod-](https://doi.org/10.18653/v1/P19-1176)
[els for machine translation. In Proceedings of](https://doi.org/10.18653/v1/P19-1176)
_the 57th Annual Meeting of the Association for_
_Computational Linguistics (ACL)._

Colin Wei, Yining Chen, and Tengyu Ma. 2022a.

[Statistically meaningful approximation: a case](https://papers.nips.cc/paper_files/paper/2022/hash/4ebf1d74f53ece08512a23309d58df89-Abstract-Conference.html)
[study on approximating Turing machines with](https://papers.nips.cc/paper_files/paper/2022/hash/4ebf1d74f53ece08512a23309d58df89-Abstract-Conference.html)
[transformers. In Advances in Neural Informa-](https://papers.nips.cc/paper_files/paper/2022/hash/4ebf1d74f53ece08512a23309d58df89-Abstract-Conference.html)
_tion Processing Systems 35 (NeurIPS), pages_
12071â€“12083.

Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
[Quoc V. Le, and Denny Zhou. 2022b. Chain-](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
[of-thought prompting elicits reasoning in large](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
[language models. In Advances in Neural Infor-](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
_mation Processing Systems 35 (NeurIPS), pages_
24824â€“24837.

Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018.

[On the practical computational power of finite](https://doi.org/10.18653/v1/P18-2117)
[precision RNNs for language recognition. In](https://doi.org/10.18653/v1/P18-2117)
_Proceedings of the 56th Annual Meeting of_
_the Association for Computational Linguistics_
_(ACL), pages 740â€“745._

Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.

[Thinking like Transformers. In Proceedings of](https://proceedings.mlr.press/v139/weiss21a.html)
_the 38th International Conference on Machine_
_Learning (ICML), volume 139 of Proceedings_
_of Machine Learning Research, pages 11080â€“_
11090.

Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. 2021. [Self-](https://doi.org/10.18653/v1/2021.acl-long.292)
[attention networks can process bounded hier-](https://doi.org/10.18653/v1/2021.acl-long.292)
[archical languages. In Proceedings of the 59th](https://doi.org/10.18653/v1/2021.acl-long.292)
_Annual Meeting of the Association for Compu-_
_tational Linguistics and the 11th International_
_Joint Conference on Natural Language Process-_
_ing (ACL-IJCNLP), pages 3770â€“3785._


17


-----

Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
Rawat, Sashank J. Reddi, and Sanjiv Kumar.
[2020. Are Transformers universal approxima-](https://openreview.net/forum?id=ByxRM0Ntvr)
[tors of sequence-to-sequence functions? In 8th](https://openreview.net/forum?id=ByxRM0Ntvr)
_International Conference on Learning Represen-_
_tations (ICLR)._

Hattie Zhou, Arwen Bradley, Etai Littwin, Noam
Razin, Omid Saremi, Josh Susskind, Samy Ben[gio, and Preetum Nakkiran. 2024. What algo-](https://openreview.net/forum?id=AssIuHnmHX)
[rithms can Transformers learn? A study in length](https://openreview.net/forum?id=AssIuHnmHX)
[generalization. In Proceedings of the Twelfth](https://openreview.net/forum?id=AssIuHnmHX)
_International Conference on Learning Represen-_
_tations (ICLR)._


18


-----

